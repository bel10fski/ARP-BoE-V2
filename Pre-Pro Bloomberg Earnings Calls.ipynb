{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Bloomberg Earnings Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script demonstrates the pre-processing of earnings call transcripts for two insurance companies, Travelers Cos (TRV) and St. James's Place, preparing them for further NLP analysis. Each transcript contains two main sections: MD (Management Discussion) / Presentation and QA (Questions and Answers). The consistent format of these transcripts, downloaded from Bloomberg, makes them ideal for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting text out of the PDFs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved text for ARP BOE\\Insurers\\ST. JAMES place\\Bloomberg\\St James_s Place PLC Earnings Call 2019731 SD000000002886567163.pdf to Earnings Calls Texts\\ST._JAMES_place\\St James_s Place PLC Earnings Call 2019731 SD000000002886567163.txt\n",
      "Saved text for ARP BOE\\Insurers\\ST. JAMES place\\Bloomberg\\St James_s Place PLC Earnings Call 2020227 DN000000002799172133.pdf to Earnings Calls Texts\\ST._JAMES_place\\St James_s Place PLC Earnings Call 2020227 DN000000002799172133.txt\n",
      "Saved text for ARP BOE\\Insurers\\ST. JAMES place\\Bloomberg\\St James_s Place PLC Earnings Call 2020728 DN000000002875532448.pdf to Earnings Calls Texts\\ST._JAMES_place\\St James_s Place PLC Earnings Call 2020728 DN000000002875532448.txt\n",
      "Saved text for ARP BOE\\Insurers\\ST. JAMES place\\Bloomberg\\St James_s Place PLC Earnings Call 2021225 RT000000002951492856.pdf to Earnings Calls Texts\\ST._JAMES_place\\St James_s Place PLC Earnings Call 2021225 RT000000002951492856.txt\n",
      "Saved text for ARP BOE\\Insurers\\ST. JAMES place\\Bloomberg\\St James_s Place PLC Earnings Call 2021728 RT000000002961195990.pdf to Earnings Calls Texts\\ST._JAMES_place\\St James_s Place PLC Earnings Call 2021728 RT000000002961195990.txt\n",
      "Saved text for ARP BOE\\Insurers\\ST. JAMES place\\Bloomberg\\St James_s Place PLC Earnings Call 2022224 DN000000002974539999.pdf to Earnings Calls Texts\\ST._JAMES_place\\St James_s Place PLC Earnings Call 2022224 DN000000002974539999.txt\n",
      "Saved text for ARP BOE\\Insurers\\ST. JAMES place\\Bloomberg\\St James_s Place PLC Earnings Call 2022728 DN000000002988128310.pdf to Earnings Calls Texts\\ST._JAMES_place\\St James_s Place PLC Earnings Call 2022728 DN000000002988128310.txt\n",
      "Saved text for ARP BOE\\Insurers\\ST. JAMES place\\Bloomberg\\St James_s Place PLC Earnings Call 2023228 DN000000003004320905.pdf to Earnings Calls Texts\\ST._JAMES_place\\St James_s Place PLC Earnings Call 2023228 DN000000003004320905.txt\n",
      "Saved text for ARP BOE\\Insurers\\ST. JAMES place\\Bloomberg\\St James_s Place PLC Earnings Call 2023727 DN000000003017788687.pdf to Earnings Calls Texts\\ST._JAMES_place\\St James_s Place PLC Earnings Call 2023727 DN000000003017788687.txt\n",
      "Saved text for ARP BOE\\Insurers\\ST. JAMES place\\Bloomberg\\St James_s Place PLC Earnings Call 2024228 DN000000003032811522.pdf to Earnings Calls Texts\\ST._JAMES_place\\St James_s Place PLC Earnings Call 2024228 DN000000003032811522.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 20191022 DN000000002732661665.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 20191022 DN000000002732661665.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2019418 DN000000002625973556.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2019418 DN000000002625973556.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2019723 RT000000002897838852.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2019723 RT000000002897838852.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 20201020 DN000000002919519869.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 20201020 DN000000002919519869.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2020123 DN000000002782019895.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2020123 DN000000002782019895.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2020421 RT000000002827435712.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2020421 RT000000002827435712.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 20211019 DN000000002966110191.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 20211019 DN000000002966110191.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2021121 RT000000002948798175.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2021121 RT000000002948798175.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2021420 DN000000002958474957.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2021420 DN000000002958474957.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2021720 RT000000002960746057.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2021720 RT000000002960746057.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 20221019 DN000000002995149139.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 20221019 DN000000002995149139.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2022120 RT000000002973194137.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2022120 RT000000002973194137.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2022419 RT000000002979120029.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2022419 RT000000002979120029.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2022721 RT000000002987761887.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2022721 RT000000002987761887.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 20231018 DN000000003023919633.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 20231018 DN000000003023919633.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2023124 RT000000003006445379.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2023124 RT000000003006445379.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2023419 DN000000003009088712.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2023419 DN000000003009088712.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2023720 DN000000003017167658.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2023720 DN000000003017167658.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2024119 RT000000003030171756.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2024119 RT000000003030171756.txt\n",
      "Saved text for ARP BOE\\Insurers\\Traveler Cos TRV\\bloomberg\\Travelers Cos IncThe Earnings Call 2024417 DN000000003036347706.pdf to Earnings Calls Texts\\Traveler_Cos_TRV\\Travelers Cos IncThe Earnings Call 2024417 DN000000003036347706.txt\n",
      "Total size of processed files: 25595918 bytes\n"
     ]
    }
   ],
   "source": [
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            if reader.is_encrypted:\n",
    "                reader.decrypt('')\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# Function to traverse the directory and extract text from PDFs of specified companies in bloomberg folders\n",
    "def traverse_and_extract_text(root_dir, output_dir, companies):\n",
    "    total_size = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        if 'bloomberg' in root.lower():  # Check if 'bloomberg' is in the directory path\n",
    "            for sector, company_list in companies.items():\n",
    "                for company in company_list:\n",
    "                    if company.lower() in root.lower():\n",
    "                        for file in files:\n",
    "                            if file.endswith(\".pdf\"):\n",
    "                                pdf_path = os.path.join(root, file)\n",
    "                                file_size = os.path.getsize(pdf_path)\n",
    "                                text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "                                if text:\n",
    "                                    # Create subdirectory for the company if it doesn't exist\n",
    "                                    company_dir = os.path.join(output_dir, company.replace(\" \", \"_\"))\n",
    "                                    if not os.path.exists(company_dir):\n",
    "                                        os.makedirs(company_dir)\n",
    "\n",
    "                                    # Save the text as a .txt file in the company subdirectory\n",
    "                                    output_file = os.path.join(company_dir, f\"{os.path.splitext(file)[0]}.txt\")\n",
    "                                    with open(output_file, 'w', encoding='utf-8') as txt_file:\n",
    "                                        txt_file.write(text)\n",
    "                                    print(f\"Saved text for {pdf_path} to {output_file}\")\n",
    "\n",
    "                                    total_size += file_size\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# Root directory path\n",
    "root_dir = \"ARP BOE\"\n",
    "# Output directory path\n",
    "output_dir = \"Earnings Calls Texts\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Specified companies\n",
    "companies = {\n",
    "    \"Insurers\": [\"Traveler Cos TRV\", \"ST. JAMES place\"],\n",
    "}\n",
    "\n",
    "# Extract text from PDFs and save them\n",
    "total_size_processed = traverse_and_extract_text(root_dir, output_dir, companies)\n",
    "print(f\"Total size of processed files: {total_size_processed} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Categorising Participants from Companies Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes text files to extract and categorise participants from the earnings call transcripts of each company. It defines a participants_list function to identify and list \"Company Participants\" and \"Other Participants\". The script then reads the text files, converts them to DataFrames and applies the participants_list function to extract participants. The participants are accumulated, duplicates are removed and the lists are saved as CSV files in the \"Participants CSVs\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved company participants for ST._JAMES_place to Participants CSVs\\ST._JAMES_place_company_participants.csv\n",
      "Saved other participants for ST._JAMES_place to Participants CSVs\\ST._JAMES_place_other_participants.csv\n",
      "Saved company participants for Traveler_Cos_TRV to Participants CSVs\\Traveler_Cos_TRV_company_participants.csv\n",
      "Saved other participants for Traveler_Cos_TRV to Participants CSVs\\Traveler_Cos_TRV_other_participants.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to extract participants list from the DataFrame\n",
    "def participants_list(df):\n",
    "    Participant_start_index = df.index[df.iloc[:, 0] == 'Company Participants'].tolist()\n",
    "    Participant_middle_index = df.index[df.iloc[:, 0] == 'Other Participants'].tolist()\n",
    "\n",
    "    Participant_end_index = df.index[df.iloc[:, 0] == 'Presentation'].tolist()\n",
    "    if Participant_end_index == []:\n",
    "        Participant_end_index = df.index[df.iloc[:, 0] == 'Presentation'].tolist()\n",
    "        if Participant_end_index == []:\n",
    "            Participant_end_index = df.index[df.iloc[:, 0] == 'Questions And Answers'].tolist()\n",
    "            Participant_end_index = [Participant_end_index[-1]]\n",
    "            if Participant_end_index == []:\n",
    "                Participant_end_index = df.index[df.iloc[:, 0] == 'Q&A'].tolist()\n",
    "                Participant_end_index = [Participant_end_index[-1]]\n",
    "        else:\n",
    "            Participant_end_index = [Participant_end_index[-1]]\n",
    "\n",
    "    if Participant_middle_index == []:\n",
    "        Participant_middle_index = Participant_end_index.copy()\n",
    "\n",
    "    company_participants = df.loc[Participant_start_index[0]+1:Participant_middle_index[0]-1].copy()\n",
    "    company_participants.drop(company_participants.index[company_participants.iloc[:, 0] == ''].tolist(), inplace=True)\n",
    "    company_participants = company_participants.values.tolist()\n",
    "\n",
    "    other_participants = df.loc[Participant_middle_index[0]+1:Participant_end_index[0]-1].copy()\n",
    "    other_participants.drop(other_participants.index[other_participants.iloc[:, 0] == ''].tolist(), inplace=True)\n",
    "    other_participants = other_participants.values.tolist()\n",
    "\n",
    "    return df, company_participants, other_participants\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = \"Earnings Calls Texts\"\n",
    "output_dir = \"Participants CSVs\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Initialise lists to accumulate participants for each company\n",
    "company_participants_all = {}\n",
    "other_participants_all = {}\n",
    "\n",
    "# Process each text file in the input directory\n",
    "for company in os.listdir(input_dir):\n",
    "    company_path = os.path.join(input_dir, company)\n",
    "    if os.path.isdir(company_path):\n",
    "        for filename in os.listdir(company_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                # Load the saved text file\n",
    "                file_path = os.path.join(company_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "\n",
    "                # Convert text to DataFrame\n",
    "                temp_df = pd.DataFrame(text.split('\\n'), columns=[0])\n",
    "\n",
    "                # Apply the participants_list function\n",
    "                _, company_participants, other_participants = participants_list(temp_df)\n",
    "                \n",
    "                # Initialize lists if not already present for the company\n",
    "                if company not in company_participants_all:\n",
    "                    company_participants_all[company] = []\n",
    "                if company not in other_participants_all:\n",
    "                    other_participants_all[company] = []\n",
    "\n",
    "                # Accumulate participants for the company\n",
    "                company_participants_all[company].extend(company_participants)\n",
    "                other_participants_all[company].extend(other_participants)\n",
    "\n",
    "# Save all participants to separate CSV files for each company\n",
    "for company in company_participants_all:\n",
    "    company_df = pd.DataFrame(company_participants_all[company], columns=['Company Participants']).drop_duplicates().reset_index(drop=True)\n",
    "    other_df = pd.DataFrame(other_participants_all[company], columns=['Other Participants']).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    company_csv_path = os.path.join(output_dir, f'{company}_company_participants.csv')\n",
    "    other_csv_path = os.path.join(output_dir, f'{company}_other_participants.csv')\n",
    "\n",
    "    company_df.to_csv(company_csv_path, index=False)\n",
    "    other_df.to_csv(other_csv_path, index=False)\n",
    "\n",
    "    print(f\"Saved company participants for {company} to {company_csv_path}\")\n",
    "    print(f\"Saved other participants for {company} to {other_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Extracting Information from Company Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes text files to extract key information and clean the text for further analysis. It features two main functions: extract_info, which retrieves the date, company name, and ticker from the text, and cleaning_text, which removes unnecessary content and formats the transcript data. The script reads the text files, extracts relevant details, applies cleaning transformations, and then saves the cleaned text to a new directory. Additionally, it uses pre-extracted lists of participants to enhance the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: St James_s Place PLC Earnings Call 2019731 SD000000002886567163.txt to Cleaned Earnings Calls Texts\\ST._JAMES_place\\cleaned_St James_s Place PLC Earnings Call 2019731 SD000000002886567163.txt\n",
      "Saved cleaned text for file: St James_s Place PLC Earnings Call 2020227 DN000000002799172133.txt to Cleaned Earnings Calls Texts\\ST._JAMES_place\\cleaned_St James_s Place PLC Earnings Call 2020227 DN000000002799172133.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: St James_s Place PLC Earnings Call 2020728 DN000000002875532448.txt to Cleaned Earnings Calls Texts\\ST._JAMES_place\\cleaned_St James_s Place PLC Earnings Call 2020728 DN000000002875532448.txt\n",
      "Saved cleaned text for file: St James_s Place PLC Earnings Call 2021225 RT000000002951492856.txt to Cleaned Earnings Calls Texts\\ST._JAMES_place\\cleaned_St James_s Place PLC Earnings Call 2021225 RT000000002951492856.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: St James_s Place PLC Earnings Call 2021728 RT000000002961195990.txt to Cleaned Earnings Calls Texts\\ST._JAMES_place\\cleaned_St James_s Place PLC Earnings Call 2021728 RT000000002961195990.txt\n",
      "Saved cleaned text for file: St James_s Place PLC Earnings Call 2022224 DN000000002974539999.txt to Cleaned Earnings Calls Texts\\ST._JAMES_place\\cleaned_St James_s Place PLC Earnings Call 2022224 DN000000002974539999.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: St James_s Place PLC Earnings Call 2022728 DN000000002988128310.txt to Cleaned Earnings Calls Texts\\ST._JAMES_place\\cleaned_St James_s Place PLC Earnings Call 2022728 DN000000002988128310.txt\n",
      "Saved cleaned text for file: St James_s Place PLC Earnings Call 2023228 DN000000003004320905.txt to Cleaned Earnings Calls Texts\\ST._JAMES_place\\cleaned_St James_s Place PLC Earnings Call 2023228 DN000000003004320905.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: St James_s Place PLC Earnings Call 2023727 DN000000003017788687.txt to Cleaned Earnings Calls Texts\\ST._JAMES_place\\cleaned_St James_s Place PLC Earnings Call 2023727 DN000000003017788687.txt\n",
      "Saved cleaned text for file: St James_s Place PLC Earnings Call 2024228 DN000000003032811522.txt to Cleaned Earnings Calls Texts\\ST._JAMES_place\\cleaned_St James_s Place PLC Earnings Call 2024228 DN000000003032811522.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 20191022 DN000000002732661665.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 20191022 DN000000002732661665.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2019418 DN000000002625973556.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2019418 DN000000002625973556.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2019723 RT000000002897838852.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2019723 RT000000002897838852.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 20201020 DN000000002919519869.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 20201020 DN000000002919519869.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2020123 DN000000002782019895.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2020123 DN000000002782019895.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2020421 RT000000002827435712.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2020421 RT000000002827435712.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 20211019 DN000000002966110191.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 20211019 DN000000002966110191.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2021121 RT000000002948798175.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2021121 RT000000002948798175.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2021420 DN000000002958474957.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2021420 DN000000002958474957.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2021720 RT000000002960746057.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2021720 RT000000002960746057.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 20221019 DN000000002995149139.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 20221019 DN000000002995149139.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2022120 RT000000002973194137.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2022120 RT000000002973194137.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2022419 RT000000002979120029.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2022419 RT000000002979120029.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2022721 RT000000002987761887.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2022721 RT000000002987761887.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 20231018 DN000000003023919633.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 20231018 DN000000003023919633.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2023124 RT000000003006445379.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2023124 RT000000003006445379.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2023419 DN000000003009088712.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2023419 DN000000003009088712.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2023720 DN000000003017167658.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2023720 DN000000003017167658.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2024119 RT000000003030171756.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2024119 RT000000003030171756.txt\n",
      "Saved cleaned text for file: Travelers Cos IncThe Earnings Call 2024417 DN000000003036347706.txt to Cleaned Earnings Calls Texts\\Traveler_Cos_TRV\\cleaned_Travelers Cos IncThe Earnings Call 2024417 DN000000003036347706.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0] = df[0].apply(remove_participant_names)\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:104: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
      "C:\\Users\\dimi3\\AppData\\Local\\Temp\\ipykernel_11144\\3629757075.py:106: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract date, company name, and ticker from the text\n",
    "def extract_info(text):\n",
    "    date_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n",
    "    company_ticker_pattern = re.compile(r'(.*\\s+\\(\\w+\\s+Equity\\))')\n",
    "\n",
    "    date_match = date_pattern.search(text)\n",
    "    company_ticker_match = company_ticker_pattern.search(text)\n",
    "\n",
    "    date = date_match.group(0) if date_match else ''\n",
    "    company_ticker = company_ticker_match.group(1) if company_ticker_match else ''\n",
    "\n",
    "    company_name, ticker = '', ''\n",
    "    if company_ticker:\n",
    "        parts = company_ticker.rsplit('(', 1)\n",
    "        company_name = parts[0].strip()\n",
    "        ticker = parts[1].strip(')')\n",
    "\n",
    "    return date, company_name, ticker\n",
    "\n",
    "# Cleaning text function\n",
    "def cleaning_text(df, date, company_name, ticker, company_participants_list, other_participants_list):\n",
    "    # Remove the unnecessary string\n",
    "    df[0] = df[0].str.replace('\\n', '', regex=False)\n",
    "    df[0] = df[0].str.replace('TRANSCRIPT', '', regex=False)\n",
    "    df[0] = df[0].str.replace('\\x0c\\n', '', regex=False)\n",
    "    df[0] = df[0].str.replace('FINAL', '', regex=False)\n",
    "    df[0] = df[0].str.replace('*', '', regex=False)\n",
    "    df[0] = df[0].str.replace('[', '', regex=False)\n",
    "    df[0] = df[0].str.replace(']', '', regex=False)\n",
    "    df[0] = df[0].str.replace(':', '', regex=False)\n",
    "    df[0] = df[0].str.replace('A - ', '', regex=False)\n",
    "    df[0] = df[0].str.replace('Q - ', '', regex=False)\n",
    "    df[0] = df[0].str.replace('{BIO', '', regex=False)\n",
    "    df[0] = df[0].str.replace('}', '', regex=False)\n",
    "    df[0] = df[0].str.replace('<', '', regex=False)\n",
    "    df[0] = df[0].str.replace('>', '', regex=False)\n",
    "    df[0] = df[0].str.replace('GO', '', regex=False)\n",
    "\n",
    "    # Remove occurrences of date, company name, and ticker\n",
    "    df[0] = df[0].str.replace(date, '', regex=False)\n",
    "    df[0] = df[0].str.replace(f\"{company_name} ({ticker})\", '', regex=False)\n",
    "    \n",
    "    # Drop rows that start with a 7 or 8-digit number\n",
    "    df = df[~df[0].str.match(r'^\\s*\\d{7,8}', na=False)]\n",
    "\n",
    "    # Remove names from the beginning of rows if they match any names in the lists\n",
    "    all_participants_list = company_participants_list + other_participants_list\n",
    "    def remove_participant_names(row):\n",
    "        for name in all_participants_list:\n",
    "            if row.startswith(name):\n",
    "                return row[len(name):].strip()\n",
    "        return row\n",
    "\n",
    "    df[0] = df[0].apply(remove_participant_names)\n",
    "\n",
    "    # Remove rows that contain 'Equity' within parentheses and move it below the date if it exists\n",
    "    equity_rows = df[0].str.contains(r'\\(.*Equity.*\\)', na=False)\n",
    "    if equity_rows.any():\n",
    "        equity_info = df[equity_rows].iloc[0, 0]  # Extract the equity info\n",
    "        df = df[~equity_rows]  # Remove the equity rows\n",
    "    else:\n",
    "        equity_info = ''\n",
    "\n",
    "    # Drop rows that start with 'Operator'\n",
    "    df = df[~df[0].str.match(r'^\\s*Operator', na=False)]\n",
    "\n",
    "    # After extracting the participants, we can drop those sections to make the transcript clearer\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Drop rows from the third row till 'Presentation'\n",
    "    presentation_index = df.index[df.iloc[:, 0].str.contains('Presentation')].tolist()\n",
    "    if presentation_index:\n",
    "        df = df.drop(range(2, presentation_index[0]))\n",
    "    # Drop the row that ends with 'Investor Day' using regex\n",
    "    df = df[~df[0].str.contains(r'Investor Day$')]\n",
    "    # Drop the first row of the df\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.iloc[1:, :]\n",
    "    # Reset the index again to make sure the index is continuous for better processing\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Using re to remove the unnecessary string\n",
    "    def drop_unnecessary(x):\n",
    "        page = re.findall(r'Page \\d+ of \\d+', x)\n",
    "        Company_Name = re.findall(r'Company Name', x)\n",
    "        Company_Ticker = re.findall(r'Company Ticker', x)\n",
    "        Date = re.findall(r'Date', x)\n",
    "        if page == [] and Company_Name == [] and Company_Ticker == [] and Date == []:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    true_false = df[0].apply(lambda x: drop_unnecessary(x))\n",
    "    df = df[true_false]\n",
    "\n",
    "    # Drop the final page declaration\n",
    "    df = df[df[0] != 'This transcript may not be 100 percent accurate and may contain misspellings and ']\n",
    "    df = df[df[0] != 'other inaccuracies. This transcript is provided \"as is\", without express or implied ']\n",
    "    df = df[df[0] != 'warranties of any kind. Bloomberg retains all rights to this transcript and provides it ']\n",
    "    df = df[df[0] != 'solely for your personal, non-commercial use. Bloomberg, its suppliers and third-']\n",
    "    df = df[~df[0].str.contains(r'solely for your personal, non-commercial use\\. Bloomberg, its suppliers and third-', regex=True)]\n",
    "    df = df[df[0] != 'personal, non-commercial use. Bloomberg, its suppliers and third-party agents shall ']\n",
    "    df = df[df[0] != 'party agents shall have no liability for errors in this transcript or for lost prots, losses, ']\n",
    "    df = df[df[0] != 'or direct, indirect, incidental, consequential, special or punitive damages in']\n",
    "    df = df[~df[0].str.contains(r'or direct, indirect, incidental, consequential, special or punitive damages in(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
    "    df = df[df[0] != 'connection with the furnishing, performance or use of such transcript. Neither the ']\n",
    "    df = df[~df[0].str.contains(r'connection with the furnishing, performance or use of such transcript\\. Neither the(\\s+TRANSCRIPT\\s+\\d{4}-\\d{2}-\\d{2})?', regex=True)]\n",
    "    df = df[df[0] != 'information nor any opinion expressed in this transcript constitutes a solicitation of ']\n",
    "    df = df[df[0] != 'the purchase or sale of securities or commodities. Any opinion expressed in the ']\n",
    "    df = df[df[0] != 'transcript does not necessarily reect the views of Bloomberg LP.  COPYRIGHT ']\n",
    "    df = df[df[0] != '2024, BLOOMBERG LP. All rights reserved. Any reproduction, redistribution or ']\n",
    "    df = df[df[0] != 'retransmission is expressly prohibited.']\n",
    "\n",
    "    # if could not be identified, we would apply re\n",
    "    def drop_Bloomberg_mark(x):\n",
    "        Bloomberg_mark = re.findall(r'reflect the views of Bloomberg LP', x)\n",
    "        if Bloomberg_mark == []:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    true_false_bm = df[0].apply(lambda x: drop_Bloomberg_mark(x))\n",
    "    df = df[true_false_bm]\n",
    "\n",
    "    # Drop the empty row\n",
    "    df = df[df[0] != '']\n",
    "\n",
    "    # Reset the index to make sure the index is continuous for better processing\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Add date and equity info to the top of the dataframe\n",
    "    if date:\n",
    "        df = pd.concat([pd.DataFrame([[date, company_name, ticker]], columns=[0, 1, 2]), df], ignore_index=True)\n",
    "    if equity_info:\n",
    "        df = pd.concat([pd.DataFrame([[equity_info]], columns=[0]), df], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Path to the directory containing the text files\n",
    "input_dir = \"Earnings Calls Texts\"\n",
    "output_dir = \"Cleaned Earnings Calls Texts\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load participants lists from CSV files for all companies\n",
    "all_company_participants = []\n",
    "all_other_participants = []\n",
    "\n",
    "# Traverse the Participants CSVs folder to load participants for each company\n",
    "participants_dir = \"Participants CSVs\"\n",
    "for file in os.listdir(participants_dir):\n",
    "    if file.endswith(\"_company_participants.csv\"):\n",
    "        company_participants_df = pd.read_csv(os.path.join(participants_dir, file))\n",
    "        all_company_participants.extend(company_participants_df['Company Participants'].tolist())\n",
    "    elif file.endswith(\"_other_participants.csv\"):\n",
    "        other_participants_df = pd.read_csv(os.path.join(participants_dir, file))\n",
    "        all_other_participants.extend(other_participants_df['Other Participants'].tolist())\n",
    "\n",
    "# Process and clean all text files\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # Load the saved text file\n",
    "            file_path = os.path.join(root, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Extract date, company name, and ticker\n",
    "            date, company_name, ticker = extract_info(text)\n",
    "\n",
    "            # Apply the cleaning function\n",
    "            temp_df = pd.DataFrame(text.split('\\n'), columns=[0])\n",
    "            cleaned_temp_df = cleaning_text(temp_df, date, company_name, ticker, all_company_participants, all_other_participants)\n",
    "            cleaned_text = '\\n'.join(cleaned_temp_df[0].tolist())\n",
    "\n",
    "            # Determine the relative output path\n",
    "            relative_path = os.path.relpath(root, input_dir)\n",
    "            output_file_dir = os.path.join(output_dir, relative_path)\n",
    "            if not os.path.exists(output_file_dir):\n",
    "                os.makedirs(output_file_dir)\n",
    "\n",
    "            # Save the cleaned text as .txt files\n",
    "            cleaned_file_path = os.path.join(output_file_dir, f\"cleaned_{filename}\")\n",
    "            with open(cleaned_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "                txt_file.write(cleaned_text)\n",
    "\n",
    "            print(f\"Saved cleaned text for file: {filename} to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Segregating Sections from Cleaned Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes the cleaned text files to extract and segregate different sections for further analysis. For each file, it identifies and separates the 'Presentation' and 'Questions And Answers' sections. The script accumulates three types of text:\n",
    "\n",
    "- Clear_MDQA: All the text in one file containing both the Presentation and QA sections.\n",
    "- Clear_MD: Only the Presentation section.\n",
    "- Clear_QA: Only the Questions and Answers section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ST._JAMES_place_Clear_MDQA.txt to Clear Earnings Calls Texts\\ST._JAMES_place\\ST._JAMES_place_Clear_MDQA.txt\n",
      "Saved ST._JAMES_place_Clear_MD.txt to Clear Earnings Calls Texts\\ST._JAMES_place\\ST._JAMES_place_Clear_MD.txt\n",
      "Saved ST._JAMES_place_Clear_QA.txt to Clear Earnings Calls Texts\\ST._JAMES_place\\ST._JAMES_place_Clear_QA.txt\n",
      "Saved Traveler_Cos_TRV_Clear_MDQA.txt to Clear Earnings Calls Texts\\Traveler_Cos_TRV\\Traveler_Cos_TRV_Clear_MDQA.txt\n",
      "Saved Traveler_Cos_TRV_Clear_MD.txt to Clear Earnings Calls Texts\\Traveler_Cos_TRV\\Traveler_Cos_TRV_Clear_MD.txt\n",
      "Saved Traveler_Cos_TRV_Clear_QA.txt to Clear Earnings Calls Texts\\Traveler_Cos_TRV\\Traveler_Cos_TRV_Clear_QA.txt\n"
     ]
    }
   ],
   "source": [
    "# Directories for input and output\n",
    "input_dir = \"Cleaned Earnings Calls Texts\"\n",
    "output_dir = \"Clear Earnings Calls Texts\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Process each company folder within the input directory\n",
    "for company_folder in os.listdir(input_dir):\n",
    "    company_input_dir = os.path.join(input_dir, company_folder)\n",
    "    company_output_dir = os.path.join(output_dir, company_folder)\n",
    "    \n",
    "    # Create company output directory if it doesn't exist\n",
    "    if not os.path.exists(company_output_dir):\n",
    "        os.makedirs(company_output_dir)\n",
    "\n",
    "    # Initialise text accumulators for the current company\n",
    "    clear_mdqa_text = []\n",
    "    clear_md_text = []\n",
    "    clear_qa_text = []\n",
    "\n",
    "    # Process each text file in the company input directory\n",
    "    for root, dirs, files in os.walk(company_input_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    lines = text.split('\\n')\n",
    "                    temp_df = pd.DataFrame(lines, columns=[0])\n",
    "\n",
    "                    # Find indices for 'Presentation' and 'Questions And Answers'\n",
    "                    presentation_index = temp_df.index[temp_df[0].str.contains('Presentation', na=False)].tolist()\n",
    "                    qa_index = temp_df.index[temp_df[0].str.contains('Questions And Answers', na=False)].tolist()\n",
    "\n",
    "                    # Append to clear_mdqa_text (excluding 'Presentation' and 'Questions And Answers' rows)\n",
    "                    for line in lines:\n",
    "                        if 'Presentation' not in line and 'Questions And Answers' not in line:\n",
    "                            clear_mdqa_text.append(line)\n",
    "\n",
    "                    # Append to clear_md_text (from below 'Presentation' to above 'Questions And Answers')\n",
    "                    if presentation_index and qa_index:\n",
    "                        clear_md_text.extend(lines[presentation_index[0]+1:qa_index[0]])\n",
    "\n",
    "                    # Append to clear_qa_text (from below 'Questions And Answers' to end)\n",
    "                    if qa_index:\n",
    "                        clear_qa_text.extend(lines[qa_index[0]+1:])\n",
    "\n",
    "    # Save accumulated texts to respective files for the current company\n",
    "    clear_mdqa_path = os.path.join(company_output_dir, f'{company_folder}_Clear_MDQA.txt')\n",
    "    with open(clear_mdqa_path, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(clear_mdqa_text))\n",
    "\n",
    "    clear_md_path = os.path.join(company_output_dir, f'{company_folder}_Clear_MD.txt')\n",
    "    with open(clear_md_path, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(clear_md_text))\n",
    "\n",
    "    clear_qa_path = os.path.join(company_output_dir, f'{company_folder}_Clear_QA.txt')\n",
    "    with open(clear_qa_path, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(clear_qa_text))\n",
    "\n",
    "    print(f\"Saved {company_folder}_Clear_MDQA.txt to {clear_mdqa_path}\")\n",
    "    print(f\"Saved {company_folder}_Clear_MD.txt to {clear_md_path}\")\n",
    "    print(f\"Saved {company_folder}_Clear_QA.txt to {clear_qa_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Earnings Call Transcripts into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code reads the text files, filtering out lines starting and ending with \"Presentation\" or \"Questions And Answers,\" and stores the cleaned lines in a dictionary. Additionally, it creates a DataFrame with file names as columns and lines of text as rows, ensuring each column has an equal number of rows by padding shorter files with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2019731 SD000000002886567163.txt</th>\n",
       "      <th>ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2020227 DN000000002799172133.txt</th>\n",
       "      <th>ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2020728 DN000000002875532448.txt</th>\n",
       "      <th>ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2021225 RT000000002951492856.txt</th>\n",
       "      <th>ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2021728 RT000000002961195990.txt</th>\n",
       "      <th>ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2022224 DN000000002974539999.txt</th>\n",
       "      <th>ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2022728 DN000000002988128310.txt</th>\n",
       "      <th>ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2023228 DN000000003004320905.txt</th>\n",
       "      <th>ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2023727 DN000000003017788687.txt</th>\n",
       "      <th>ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2024228 DN000000003032811522.txt</th>\n",
       "      <th>...</th>\n",
       "      <th>Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 20221019 DN000000002995149139.txt</th>\n",
       "      <th>Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2022120 RT000000002973194137.txt</th>\n",
       "      <th>Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2022419 RT000000002979120029.txt</th>\n",
       "      <th>Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2022721 RT000000002987761887.txt</th>\n",
       "      <th>Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 20231018 DN000000003023919633.txt</th>\n",
       "      <th>Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2023124 RT000000003006445379.txt</th>\n",
       "      <th>Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2023419 DN000000003009088712.txt</th>\n",
       "      <th>Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2023720 DN000000003017167658.txt</th>\n",
       "      <th>Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2024119 RT000000003030171756.txt</th>\n",
       "      <th>Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2024417 DN000000003036347706.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>...</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>2022-02-24</td>\n",
       "      <td>2022-07-28</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>2023-07-27</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-10-19</td>\n",
       "      <td>2022-01-20</td>\n",
       "      <td>2022-04-19</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>2023-10-18</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>2023-04-19</td>\n",
       "      <td>2023-07-20</td>\n",
       "      <td>2024-01-19</td>\n",
       "      <td>2024-04-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Should we get started? So good morning, everyo...</td>\n",
       "      <td>Good morning everyone. It's half ten, so we sh...</td>\n",
       "      <td>Good morning everyone, and welcome to our 2020...</td>\n",
       "      <td>Good morning. I hope you're keeping safe and w...</td>\n",
       "      <td>Good morning, and welcome to our 2021 Interim ...</td>\n",
       "      <td>Good morning and welcome to our Results Webcas...</td>\n",
       "      <td>Good morning, and welcome to our 2022 Half-Yea...</td>\n",
       "      <td>Good morning, and welcome to our Full-Year Res...</td>\n",
       "      <td>Good morning, and welcome to our 2023 Half-Yea...</td>\n",
       "      <td>Good morning, everyone. It's my pleasure to ta...</td>\n",
       "      <td>...</td>\n",
       "      <td>Good morning, ladies and gentlemen. Welcome to...</td>\n",
       "      <td>Good morning, ladies and gentlemen, and welcom...</td>\n",
       "      <td>Good morning, ladies and gentlemen. Welcome to...</td>\n",
       "      <td>Good morning, ladies and gentlemen. Welcome to...</td>\n",
       "      <td>Good morning, ladies and gentlemen. Welcome to...</td>\n",
       "      <td>Good morning, ladies and gentlemen. Welcome to...</td>\n",
       "      <td>Good morning, ladies and gentlemen. Welcome to...</td>\n",
       "      <td>Good morning, ladies and gentlemen. Welcome to...</td>\n",
       "      <td>Good morning, ladies and gentlemen. Welcome to...</td>\n",
       "      <td>Good morning, ladies and gentlemen. Welcome to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presentation. Adopting our usual format at the...</td>\n",
       "      <td>many familiar faces here today and thank you f...</td>\n",
       "      <td>Given COVID-19, today's presentation has been ...</td>\n",
       "      <td>today's presentation has been pre-recorded, an...</td>\n",
       "      <td>run through the ows, funds under management, ...</td>\n",
       "      <td>again been pre-recorded and we'll be hosting a...</td>\n",
       "      <td>today's presentation has been prerecorded, and...</td>\n",
       "      <td>undoubtedly another extraordinary year. After ...</td>\n",
       "      <td>presentation has been pre-recorded, and we wil...</td>\n",
       "      <td>presentation as CEO of St. James's Place. We h...</td>\n",
       "      <td>...</td>\n",
       "      <td>Teleconference for Travelers. We ask that you ...</td>\n",
       "      <td>Teleconference for Travelers. We ask that you ...</td>\n",
       "      <td>Teleconference for Travelers. (Operator Instru...</td>\n",
       "      <td>Teleconference for Travelers. We ask that you ...</td>\n",
       "      <td>Teleconference for Travelers. We ask that you ...</td>\n",
       "      <td>Teleconference for Travelers. We ask that you ...</td>\n",
       "      <td>Teleconference for Travelers. We ask that you ...</td>\n",
       "      <td>Teleconference for Travelers. We ask that you ...</td>\n",
       "      <td>Teleconference for Travelers. We ask that you ...</td>\n",
       "      <td>Teleconference for Travelers. We ask that you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hand over to Craig to run through the nancial...</td>\n",
       "      <td>presentation of the new decade will follow tha...</td>\n",
       "      <td>a live Q&amp;A at 10.45 AM. The agenda for this mo...</td>\n",
       "      <td>a.m. This morning's meeting will be in three s...</td>\n",
       "      <td>handing over to Craig, to cover the nancial r...</td>\n",
       "      <td>In 2020, at the height of the pandemic, St. Ja...</td>\n",
       "      <td>AM. The agenda for this morning. In a moment, ...</td>\n",
       "      <td>geopolitical conditions across the globe quick...</td>\n",
       "      <td>This morning's session will follow a familiar ...</td>\n",
       "      <td>include how we're dealing with two historical ...</td>\n",
       "      <td>...</td>\n",
       "      <td>of formal remarks at which time you will be gi...</td>\n",
       "      <td>of formal remarks. At which time, you will be ...</td>\n",
       "      <td>being recorded on April 19, 2022.</td>\n",
       "      <td>of formal remarks, at which time you will be g...</td>\n",
       "      <td>of formal remarks, at which time, you will be ...</td>\n",
       "      <td>of formal remarks, at which time you will be g...</td>\n",
       "      <td>of formal remarks, at which time you will be g...</td>\n",
       "      <td>of the formal remarks, at which time we will b...</td>\n",
       "      <td>of formal remarks, at which time you will be g...</td>\n",
       "      <td>of formal remarks, at which time you will be g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>developments and outlook. We'll follow this wi...</td>\n",
       "      <td>hand over to Craig to run through the nancial...</td>\n",
       "      <td>followed by Craig running through the nancial...</td>\n",
       "      <td>focus on the future before I provide a brief s...</td>\n",
       "      <td>outlook, and lead the Q&amp;A session, with the fu...</td>\n",
       "      <td>resilience during dicult circumstances. Thank...</td>\n",
       "      <td>gures, then hand over to Craig to cover the ...</td>\n",
       "      <td>as the year progressed. We had to contend with...</td>\n",
       "      <td>of our new business gures and how we prepared...</td>\n",
       "      <td>throughout this presentation how fundamentally...</td>\n",
       "      <td>...</td>\n",
       "      <td>answer session. As a reminder, this conference...</td>\n",
       "      <td>answer session. As a reminder, this conference...</td>\n",
       "      <td>At this time, I would like to turn the confere...</td>\n",
       "      <td>answer session. As a reminder, this conference...</td>\n",
       "      <td>answer session. As a reminder, this conference...</td>\n",
       "      <td>answer session. As a reminder, this conference...</td>\n",
       "      <td>answer session. As a reminder, this conference...</td>\n",
       "      <td>and-answer session. As a reminder, this confer...</td>\n",
       "      <td>answer session. As a reminder, this conference...</td>\n",
       "      <td>answer session. As a reminder, this conference...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "      <td>other key matters of note. I have a number of ...</td>\n",
       "      <td>partnership is adapted to COVID-19, before I w...</td>\n",
       "      <td>2020 was an extraordinary year for individuals...</td>\n",
       "      <td>Having announced on our strategic goals in Feb...</td>\n",
       "      <td>and employees, but importantly also the invest...</td>\n",
       "      <td>cover a number of other topics, the continued ...</td>\n",
       "      <td>and the conict in Ukraine which combined to c...</td>\n",
       "      <td>regime. I will then hand over to Craig who wil...</td>\n",
       "      <td>of this businesses. We continue to attract str...</td>\n",
       "      <td>...</td>\n",
       "      <td>2022.</td>\n",
       "      <td>2022.</td>\n",
       "      <td>Vice President of Investor Relations. Ms.Golds...</td>\n",
       "      <td>At this time, I would like to turn the confere...</td>\n",
       "      <td>2023.</td>\n",
       "      <td>At this time, I would like to turn the confere...</td>\n",
       "      <td>At this time, I would like to turn the confere...</td>\n",
       "      <td>2023.</td>\n",
       "      <td>2024.</td>\n",
       "      <td>At this time, I would like to turn the confere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Please do look them up over coee at the end.</td>\n",
       "      <td>colleagues here today and they are very welcome.</td>\n",
       "      <td>The rst six months of 2020 has been an extrao...</td>\n",
       "      <td>have been disrupted and we've all needed to ad...</td>\n",
       "      <td>Capital Market event in May. Today's interim r...</td>\n",
       "      <td>over the last few years. Whilst 2021 was anoth...</td>\n",
       "      <td>prospects for an advised business like St. Jam...</td>\n",
       "      <td>And in the UK this was compounded by political...</td>\n",
       "      <td>results. We will then be back to me where I'll...</td>\n",
       "      <td>management and deliver robust underlying nanc...</td>\n",
       "      <td>...</td>\n",
       "      <td>At this time, I would like to turn the confere...</td>\n",
       "      <td>At this time, I would like to turn the confere...</td>\n",
       "      <td>Thank you. Good morning, and welcome to Travel...</td>\n",
       "      <td>Vice President of Investor Relations. Ms.Golds...</td>\n",
       "      <td>At this time, I would like to turn the confere...</td>\n",
       "      <td>Vice President of Investor Relations. Ms.Golds...</td>\n",
       "      <td>Vice President of Investor Relations. Ms. Gold...</td>\n",
       "      <td>At this time. I would like to turn the confere...</td>\n",
       "      <td>At this time, I would like to turn the confere...</td>\n",
       "      <td>Vice President of Investor Relations. Ms. Gold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So the rst six months. It's fair to say that ...</td>\n",
       "      <td>Now, last year was a challenging year for the ...</td>\n",
       "      <td>and across the world. A six-months period of t...</td>\n",
       "      <td>distancing and had to embrace technology. Our ...</td>\n",
       "      <td>shorter than normal, which will leave more tim...</td>\n",
       "      <td>continuing to navigate lockdowns and disruptio...</td>\n",
       "      <td>performed during other dicult market conditio...</td>\n",
       "      <td>environment 2022 marked the second-best year f...</td>\n",
       "      <td>progress against our business priorities and h...</td>\n",
       "      <td>challenging market conditions.</td>\n",
       "      <td>...</td>\n",
       "      <td>Vice President of Investor Relations. Ms. Gold...</td>\n",
       "      <td>Vice President of Investor Relations. Ms.Golds...</td>\n",
       "      <td>2022 results. We released our press release, ...</td>\n",
       "      <td>quarter 2022 results. We released our press re...</td>\n",
       "      <td>Vice President of Investor Relations. Ms. Gold...</td>\n",
       "      <td>Thank you. Good morning, and welcome to Travel...</td>\n",
       "      <td>20602454</td>\n",
       "      <td>Vice-President of Investor Relations. Ms. Gold...</td>\n",
       "      <td>Vice President of Investor Relations. Ms.Golds...</td>\n",
       "      <td>Thank you. Good morning, and welcome to Travel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unprecedented political uncertainty in the U.K...</td>\n",
       "      <td>investor sentiment being impacted by the uncer...</td>\n",
       "      <td>began the year with renewed condence and mome...</td>\n",
       "      <td>James's Place Community have worked commendabl...</td>\n",
       "      <td>Let's start by recapping on those medium-term ...</td>\n",
       "      <td>vaccination programs saw many economies reboun...</td>\n",
       "      <td>outlook.</td>\n",
       "      <td>Place history. I will recap on these ows then...</td>\n",
       "      <td>SJP.</td>\n",
       "      <td>Digging into those headlines a little more, we...</td>\n",
       "      <td>...</td>\n",
       "      <td>2022 results. We released our press release, ...</td>\n",
       "      <td>20602454</td>\n",
       "      <td>at travelers.com under the Investors section.</td>\n",
       "      <td>webcast presentation earlier this morning. All...</td>\n",
       "      <td>20602454</td>\n",
       "      <td>quarter 2022 results. We released our press re...</td>\n",
       "      <td>Thank you. Good morning and welcome to Travele...</td>\n",
       "      <td>Thank you. Good morning, and welcome to Travel...</td>\n",
       "      <td>quarter 2023 results. We released our press re...</td>\n",
       "      <td>2024 results. We released our press release, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>macroeconomic environment together with a stra...</td>\n",
       "      <td>US, China trade dispute and domestic political...</td>\n",
       "      <td>back of strong investor sentiment following th...</td>\n",
       "      <td>circumstances. They have demonstrated once aga...</td>\n",
       "      <td>February. First, we aim to grow new business b...</td>\n",
       "      <td>markets recording positive returns. Supported ...</td>\n",
       "      <td>It goes without saying that the wider macro en...</td>\n",
       "      <td>Financials before returning to provide an upda...</td>\n",
       "      <td>So starting with new business. Most apparent i...</td>\n",
       "      <td>billion in client funds. Our partnership conti...</td>\n",
       "      <td>...</td>\n",
       "      <td>presentation earlier this morning. All of thes...</td>\n",
       "      <td>Thank you. Good morning, and welcome to Travel...</td>\n",
       "      <td>Speaking today will be Alan Schnitzer, Chairma...</td>\n",
       "      <td>website at travelers.com under the Investors s...</td>\n",
       "      <td>Thank you. Good morning, and welcome to Travel...</td>\n",
       "      <td>webcast presentation earlier this morning. All...</td>\n",
       "      <td>2023 results. We released our press release, ...</td>\n",
       "      <td>quarter 2023 results. We released our press re...</td>\n",
       "      <td>webcast presentation earlier this morning. All...</td>\n",
       "      <td>at travelers.com under the Investors section.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>U.S. and China. So unsurprisingly, this is not...</td>\n",
       "      <td>our new business ows announced in January wer...</td>\n",
       "      <td>December 2019. We were back in the groove, so ...</td>\n",
       "      <td>that we hold there. So I'd like to start by th...</td>\n",
       "      <td>growing number of advisers and increasing prod...</td>\n",
       "      <td>environment and the desire by individuals to s...</td>\n",
       "      <td>conditions in the rst half of 2022 sharply ri...</td>\n",
       "      <td>So turning to ows. Our gures frank the stren...</td>\n",
       "      <td>and savers has been challenging. Ination has ...</td>\n",
       "      <td>investments amounting to GBP15.4 billion of gr...</td>\n",
       "      <td>...</td>\n",
       "      <td>at travelers.com under the Investors section. ...</td>\n",
       "      <td>quarter 2021 results. We released our press re...</td>\n",
       "      <td>Ocer; and our three segment Presidents, Greg ...</td>\n",
       "      <td>Speaking today will be Alan Schnitzer, Chairma...</td>\n",
       "      <td>2023 results. We released our press release, ...</td>\n",
       "      <td>website at travelers.com under the Investors s...</td>\n",
       "      <td>presentation earlier this morning. All of thes...</td>\n",
       "      <td>website at travelers.com under the Investors s...</td>\n",
       "      <td>website at travelers.com under the Investors s...</td>\n",
       "      <td>Speaking today will be Alan Schnitzer, Chairma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>management business. But despite this backdrop...</td>\n",
       "      <td>albeit, we reported a solid outcome given the ...</td>\n",
       "      <td>growth in new business during the early months...</td>\n",
       "      <td>dedication and commitment.</td>\n",
       "      <td>per annum. These ows, together with modest gr...</td>\n",
       "      <td>pleased to report that St. James's Place had a...</td>\n",
       "      <td>invasion of Ukraine and more recently, politic...</td>\n",
       "      <td>highlight the fantastic job our advisors do to...</td>\n",
       "      <td>rising pressure on household budgets and savin...</td>\n",
       "      <td>our client retention rates have stayed strong ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Chairman and CEO; Dan Frey, Chief Financial O...</td>\n",
       "      <td>webcast presentation earlier this morning. All...</td>\n",
       "      <td>Je Klenk of Bond &amp; Specialty Insurance, and M...</td>\n",
       "      <td>three segment presidents, Greg Toczydlowski of...</td>\n",
       "      <td>sorry, and webcast presentation earlier this m...</td>\n",
       "      <td>Ocer; and our three segment Presidents Greg T...</td>\n",
       "      <td>at travelers.com under the Investors section.</td>\n",
       "      <td>Speaking today will be Alan Schnitzer, Chairma...</td>\n",
       "      <td>Speaking today will be Alan Schnitzer, Chairma...</td>\n",
       "      <td>Ocer; and our three Segment Presidents, Greg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>solid set of results, once again demonstrating...</td>\n",
       "      <td>again demonstrating the strength and resilienc...</td>\n",
       "      <td>Gross inows in the rst quarter increased by ...</td>\n",
       "      <td>Operationally, the performance of the business...</td>\n",
       "      <td>see funds under management reach more than GBP...</td>\n",
       "      <td>time last year at our 2020 results presentatio...</td>\n",
       "      <td>remembering that in 2021, we experienced growt...</td>\n",
       "      <td>want. During the year our advisors attracted G...</td>\n",
       "      <td>has reacted by raising interest rates, and thi...</td>\n",
       "      <td>our relationships with clients and their cond...</td>\n",
       "      <td>...</td>\n",
       "      <td>presidents Greg Toczydlowski of Business Insur...</td>\n",
       "      <td>website at travelers.com under the Investors s...</td>\n",
       "      <td>They will discuss the nancial results of our ...</td>\n",
       "      <td>Bond &amp; Specialty Insurance; and Michael Klein ...</td>\n",
       "      <td>found on our website at travelers.com under th...</td>\n",
       "      <td>Je Klenk of Bond &amp; Specialty Insurance, and M...</td>\n",
       "      <td>Speaking today will be Alan Schnitzer, Chairma...</td>\n",
       "      <td>three segment Presidents Greg Toczydlowski of ...</td>\n",
       "      <td>Ocer; and our three segment presidents, Greg ...</td>\n",
       "      <td>Je Klenk of Bond &amp; Specialty Insurance; and M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>New gross inows for the six months was GBP 7....</td>\n",
       "      <td>Craig will provide a reminder of last month's ...</td>\n",
       "      <td>continuing strong retention, net inows for th...</td>\n",
       "      <td>disrupted. However, St. James's Place demonstr...</td>\n",
       "      <td>And third, while we will continue to invest in...</td>\n",
       "      <td>targets for gross and net ows, funds under ma...</td>\n",
       "      <td>34%, respectively. Therefore, whilst we starte...</td>\n",
       "      <td>making 2022 our second-best year for new busin...</td>\n",
       "      <td>savers and borrowers.</td>\n",
       "      <td>these resulted in net inows of GBP5.1 billion...</td>\n",
       "      <td>...</td>\n",
       "      <td>Insurance, and Michael Klein of Personal Insur...</td>\n",
       "      <td>Speaking today will be Alan Schnitzer, Chairma...</td>\n",
       "      <td>environment. They will refer to the webcast pr...</td>\n",
       "      <td>discuss nancial results of our business and t...</td>\n",
       "      <td>Speaking today will be Alan Schnitzer, Chairma...</td>\n",
       "      <td>They will discuss the nancial results of our ...</td>\n",
       "      <td>Ocer; and our three segment presidents, Greg ...</td>\n",
       "      <td>Bond &amp; Specialty Insurance; and Michael Klein ...</td>\n",
       "      <td>Je Klenk of Bond and Specialty Insurance; and...</td>\n",
       "      <td>They will discuss the nancial results of our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rst half of 2018. And encouragingly, the gros...</td>\n",
       "      <td>have to share with you my favorite slide, cons...</td>\n",
       "      <td>This represented annualized growth of 8.1% on ...</td>\n",
       "      <td>our recent major investment in technology, tog...</td>\n",
       "      <td>growth and maintain our market leading positio...</td>\n",
       "      <td>then at our Capital Markets event in May, we a...</td>\n",
       "      <td>knew the comparators were tough, and we're, th...</td>\n",
       "      <td>reasons why we've been able to achieve this de...</td>\n",
       "      <td>sluggish at best. While this paints a gloomy p...</td>\n",
       "      <td>performance driving gains of GBP14.7 billion, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>results of our business and the current market...</td>\n",
       "      <td>Ocer; and our three-segment Presidents; Greg ...</td>\n",
       "      <td>prepared remarks, and then we will take questi...</td>\n",
       "      <td>will refer to the webcast presentation as they...</td>\n",
       "      <td>Ocer; and our three segment Presidents, Greg ...</td>\n",
       "      <td>environment. They will refer to the webcast pr...</td>\n",
       "      <td>Je Klenk of Bond &amp; Specialty Insurance; and M...</td>\n",
       "      <td>discuss the nancial results of our business a...</td>\n",
       "      <td>They will discuss the nancial results of our ...</td>\n",
       "      <td>environment. They will refer to the webcast pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Quarter versus the First Quarter.</td>\n",
       "      <td>last decade. The full year net inow of some G...</td>\n",
       "      <td>Unfortunately, the momentum that was building ...</td>\n",
       "      <td>and employees. And let's just reect on that o...</td>\n",
       "      <td>we have put in place, over the last few years,...</td>\n",
       "      <td>covered other business priorities.</td>\n",
       "      <td>year-on-year growth percentages, which have be...</td>\n",
       "      <td>far the most important of these is our fully a...</td>\n",
       "      <td>are of course empathetic to the diculties hou...</td>\n",
       "      <td>grown by almost GBP20 billion in 2023.</td>\n",
       "      <td>...</td>\n",
       "      <td>webcast presentation as they go through prepar...</td>\n",
       "      <td>Je Klenk of Bond &amp; Specialty Insurance; and M...</td>\n",
       "      <td>Before I turn the call over, I'd like to draw ...</td>\n",
       "      <td>then we will take your questions.</td>\n",
       "      <td>Je Klenk of Bond &amp; Specialty Insurance; and M...</td>\n",
       "      <td>prepared remarks and then we will take your qu...</td>\n",
       "      <td>They will discuss the nancial results of our ...</td>\n",
       "      <td>They will refer to the webcast presentation as...</td>\n",
       "      <td>environment. They will refer to the webcast pr...</td>\n",
       "      <td>prepared remarks and then we will take your qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Looking back just ve years ago, gross ows fo...</td>\n",
       "      <td>opening funds under management. So, yes, a tou...</td>\n",
       "      <td>19 pandemic, which has had a profound impact o...</td>\n",
       "      <td>opening funds under management, which ended th...</td>\n",
       "      <td>exibility and eciency, such that our control...</td>\n",
       "      <td>by 10% per annum, which would be delivered by ...</td>\n",
       "      <td>And with this in mind, the new business ows w...</td>\n",
       "      <td>making long-term nancial plans and long-term ...</td>\n",
       "      <td>remember that the need for advice isn't going ...</td>\n",
       "      <td>To put that into context, after this business ...</td>\n",
       "      <td>...</td>\n",
       "      <td>your questions.</td>\n",
       "      <td>They will discuss the nancial results of our ...</td>\n",
       "      <td>included at the end of the webcast presentatio...</td>\n",
       "      <td>Before I turn the call over to Alan, I would l...</td>\n",
       "      <td>They will discuss the nancial results of our ...</td>\n",
       "      <td>Before I turn the call over to Alan, I'd like ...</td>\n",
       "      <td>environment. They will refer to the webcast pr...</td>\n",
       "      <td>then we will take questions.</td>\n",
       "      <td>prepared remarks, and then we will take your q...</td>\n",
       "      <td>Before I turn the call over to Alan, I'd like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>over that 5-year period, we have achieved comp...</td>\n",
       "      <td>outcome particularly relative to the sector. O...</td>\n",
       "      <td>and other government imposed measures have res...</td>\n",
       "      <td>GBP129.3 billion. The business continued to gr...</td>\n",
       "      <td>will be around 5% per annum.</td>\n",
       "      <td>and increasing productivity. Second, to mainta...</td>\n",
       "      <td>highest H1 in our history. The strong performa...</td>\n",
       "      <td>Even though the environment deteriorate as the...</td>\n",
       "      <td>individuals realize the challenges to achievin...</td>\n",
       "      <td>funds under management to reach GBP20 billion,...</td>\n",
       "      <td>...</td>\n",
       "      <td>Before I turn the call over to Alan, I'd like ...</td>\n",
       "      <td>environment. They will refer to the webcast pr...</td>\n",
       "      <td>forward-looking statements. The company cautio...</td>\n",
       "      <td>explanatory note included at the end of the we...</td>\n",
       "      <td>environment. They will refer to the webcast pr...</td>\n",
       "      <td>note included at the end of the webcast presen...</td>\n",
       "      <td>prepared remarks, and then we will take questi...</td>\n",
       "      <td>Before I turn the call over to Alan, I'd like ...</td>\n",
       "      <td>Before I turn the call over to Alan, I'd like ...</td>\n",
       "      <td>note included at the end of the webcast presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Importantly, the continued strong retention of...</td>\n",
       "      <td>performance in the nal quarter as investor se...</td>\n",
       "      <td>adapt rapidly developing new habits and practi...</td>\n",
       "      <td>distancing and lockdown. And given these unpre...</td>\n",
       "      <td>We then went on to say that the combination of...</td>\n",
       "      <td>thereby generating 10% growth per annum in net...</td>\n",
       "      <td>device-led business, which helps clients plan ...</td>\n",
       "      <td>savers compounded our new business held up wel...</td>\n",
       "      <td>Our advisors are facing into this by supportin...</td>\n",
       "      <td>have today. In terms of underlying nancial pe...</td>\n",
       "      <td>...</td>\n",
       "      <td>note included at the end-of-the webcast presen...</td>\n",
       "      <td>prepared remarks and then we will take questions.</td>\n",
       "      <td>looking statement involves risks and uncertain...</td>\n",
       "      <td>today includes forward-looking statements. The...</td>\n",
       "      <td>prepared remarks, and then we will take questi...</td>\n",
       "      <td>includes forward-looking statements. The compa...</td>\n",
       "      <td>Before I turn the call over to Alan, I'd like ...</td>\n",
       "      <td>note included at the end of the webcast presen...</td>\n",
       "      <td>note included at the end of the webcast presen...</td>\n",
       "      <td>includes forward-looking statements. The compa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2019731 SD000000002886567163.txt  \\\n",
       "0                St James's Place PLC (STJ LN Equity)                                            \n",
       "1                                          2019-07-31                                            \n",
       "2   Should we get started? So good morning, everyo...                                            \n",
       "3   presentation. Adopting our usual format at the...                                            \n",
       "4   hand over to Craig to run through the nancial...                                            \n",
       "5   developments and outlook. We'll follow this wi...                                            \n",
       "6   There are also a number of my executive team a...                                            \n",
       "7       Please do look them up over coee at the end.                                            \n",
       "8   So the rst six months. It's fair to say that ...                                            \n",
       "9   unprecedented political uncertainty in the U.K...                                            \n",
       "10  macroeconomic environment together with a stra...                                            \n",
       "11  U.S. and China. So unsurprisingly, this is not...                                            \n",
       "12  management business. But despite this backdrop...                                            \n",
       "13  solid set of results, once again demonstrating...                                            \n",
       "14  New gross inows for the six months was GBP 7....                                            \n",
       "15  rst half of 2018. And encouragingly, the gros...                                            \n",
       "16                Quarter versus the First Quarter.                                              \n",
       "17  Looking back just ve years ago, gross ows fo...                                            \n",
       "18  over that 5-year period, we have achieved comp...                                            \n",
       "19  Importantly, the continued strong retention of...                                            \n",
       "\n",
       "   ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2020227 DN000000002799172133.txt  \\\n",
       "0                St James's Place PLC (STJ LN Equity)                                            \n",
       "1                                          2020-02-27                                            \n",
       "2   Good morning everyone. It's half ten, so we sh...                                            \n",
       "3   many familiar faces here today and thank you f...                                            \n",
       "4   presentation of the new decade will follow tha...                                            \n",
       "5   hand over to Craig to run through the nancial...                                            \n",
       "6   other key matters of note. I have a number of ...                                            \n",
       "7    colleagues here today and they are very welcome.                                            \n",
       "8   Now, last year was a challenging year for the ...                                            \n",
       "9   investor sentiment being impacted by the uncer...                                            \n",
       "10  US, China trade dispute and domestic political...                                            \n",
       "11  our new business ows announced in January wer...                                            \n",
       "12  albeit, we reported a solid outcome given the ...                                            \n",
       "13  again demonstrating the strength and resilienc...                                            \n",
       "14  Craig will provide a reminder of last month's ...                                            \n",
       "15  have to share with you my favorite slide, cons...                                            \n",
       "16  last decade. The full year net inow of some G...                                            \n",
       "17  opening funds under management. So, yes, a tou...                                            \n",
       "18  outcome particularly relative to the sector. O...                                            \n",
       "19  performance in the nal quarter as investor se...                                            \n",
       "\n",
       "   ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2020728 DN000000002875532448.txt  \\\n",
       "0                St James's Place PLC (STJ LN Equity)                                            \n",
       "1                                          2020-07-28                                            \n",
       "2   Good morning everyone, and welcome to our 2020...                                            \n",
       "3   Given COVID-19, today's presentation has been ...                                            \n",
       "4   a live Q&A at 10.45 AM. The agenda for this mo...                                            \n",
       "5   followed by Craig running through the nancial...                                            \n",
       "6   partnership is adapted to COVID-19, before I w...                                            \n",
       "7   The rst six months of 2020 has been an extrao...                                            \n",
       "8   and across the world. A six-months period of t...                                            \n",
       "9   began the year with renewed condence and mome...                                            \n",
       "10  back of strong investor sentiment following th...                                            \n",
       "11  December 2019. We were back in the groove, so ...                                            \n",
       "12  growth in new business during the early months...                                            \n",
       "13  Gross inows in the rst quarter increased by ...                                            \n",
       "14  continuing strong retention, net inows for th...                                            \n",
       "15  This represented annualized growth of 8.1% on ...                                            \n",
       "16  Unfortunately, the momentum that was building ...                                            \n",
       "17  19 pandemic, which has had a profound impact o...                                            \n",
       "18  and other government imposed measures have res...                                            \n",
       "19  adapt rapidly developing new habits and practi...                                            \n",
       "\n",
       "   ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2021225 RT000000002951492856.txt  \\\n",
       "0                St James's Place PLC (STJ LN Equity)                                            \n",
       "1                                          2021-02-25                                            \n",
       "2   Good morning. I hope you're keeping safe and w...                                            \n",
       "3   today's presentation has been pre-recorded, an...                                            \n",
       "4   a.m. This morning's meeting will be in three s...                                            \n",
       "5   focus on the future before I provide a brief s...                                            \n",
       "6   2020 was an extraordinary year for individuals...                                            \n",
       "7   have been disrupted and we've all needed to ad...                                            \n",
       "8   distancing and had to embrace technology. Our ...                                            \n",
       "9   James's Place Community have worked commendabl...                                            \n",
       "10  circumstances. They have demonstrated once aga...                                            \n",
       "11  that we hold there. So I'd like to start by th...                                            \n",
       "12                         dedication and commitment.                                            \n",
       "13  Operationally, the performance of the business...                                            \n",
       "14  disrupted. However, St. James's Place demonstr...                                            \n",
       "15  our recent major investment in technology, tog...                                            \n",
       "16  and employees. And let's just reect on that o...                                            \n",
       "17  opening funds under management, which ended th...                                            \n",
       "18  GBP129.3 billion. The business continued to gr...                                            \n",
       "19  distancing and lockdown. And given these unpre...                                            \n",
       "\n",
       "   ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2021728 RT000000002961195990.txt  \\\n",
       "0                St James's Place PLC (STJ LN Equity)                                            \n",
       "1                                          2021-07-28                                            \n",
       "2   Good morning, and welcome to our 2021 Interim ...                                            \n",
       "3   run through the ows, funds under management, ...                                            \n",
       "4   handing over to Craig, to cover the nancial r...                                            \n",
       "5   outlook, and lead the Q&A session, with the fu...                                            \n",
       "6   Having announced on our strategic goals in Feb...                                            \n",
       "7   Capital Market event in May. Today's interim r...                                            \n",
       "8   shorter than normal, which will leave more tim...                                            \n",
       "9   Let's start by recapping on those medium-term ...                                            \n",
       "10  February. First, we aim to grow new business b...                                            \n",
       "11  growing number of advisers and increasing prod...                                            \n",
       "12  per annum. These ows, together with modest gr...                                            \n",
       "13  see funds under management reach more than GBP...                                            \n",
       "14  And third, while we will continue to invest in...                                            \n",
       "15  growth and maintain our market leading positio...                                            \n",
       "16  we have put in place, over the last few years,...                                            \n",
       "17  exibility and eciency, such that our control...                                            \n",
       "18                       will be around 5% per annum.                                            \n",
       "19  We then went on to say that the combination of...                                            \n",
       "\n",
       "   ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2022224 DN000000002974539999.txt  \\\n",
       "0                St James's Place PLC (STJ LN Equity)                                            \n",
       "1                                          2022-02-24                                            \n",
       "2   Good morning and welcome to our Results Webcas...                                            \n",
       "3   again been pre-recorded and we'll be hosting a...                                            \n",
       "4   In 2020, at the height of the pandemic, St. Ja...                                            \n",
       "5   resilience during dicult circumstances. Thank...                                            \n",
       "6   and employees, but importantly also the invest...                                            \n",
       "7   over the last few years. Whilst 2021 was anoth...                                            \n",
       "8   continuing to navigate lockdowns and disruptio...                                            \n",
       "9   vaccination programs saw many economies reboun...                                            \n",
       "10  markets recording positive returns. Supported ...                                            \n",
       "11  environment and the desire by individuals to s...                                            \n",
       "12  pleased to report that St. James's Place had a...                                            \n",
       "13  time last year at our 2020 results presentatio...                                            \n",
       "14  targets for gross and net ows, funds under ma...                                            \n",
       "15  then at our Capital Markets event in May, we a...                                            \n",
       "16               covered other business priorities.                                              \n",
       "17  by 10% per annum, which would be delivered by ...                                            \n",
       "18  and increasing productivity. Second, to mainta...                                            \n",
       "19  thereby generating 10% growth per annum in net...                                            \n",
       "\n",
       "   ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2022728 DN000000002988128310.txt  \\\n",
       "0                St James's Place PLC (STJ LN Equity)                                            \n",
       "1                                          2022-07-28                                            \n",
       "2   Good morning, and welcome to our 2022 Half-Yea...                                            \n",
       "3   today's presentation has been prerecorded, and...                                            \n",
       "4   AM. The agenda for this morning. In a moment, ...                                            \n",
       "5   gures, then hand over to Craig to cover the ...                                            \n",
       "6   cover a number of other topics, the continued ...                                            \n",
       "7   prospects for an advised business like St. Jam...                                            \n",
       "8   performed during other dicult market conditio...                                            \n",
       "9                                            outlook.                                            \n",
       "10  It goes without saying that the wider macro en...                                            \n",
       "11  conditions in the rst half of 2022 sharply ri...                                            \n",
       "12  invasion of Ukraine and more recently, politic...                                            \n",
       "13  remembering that in 2021, we experienced growt...                                            \n",
       "14  34%, respectively. Therefore, whilst we starte...                                            \n",
       "15  knew the comparators were tough, and we're, th...                                            \n",
       "16  year-on-year growth percentages, which have be...                                            \n",
       "17  And with this in mind, the new business ows w...                                            \n",
       "18  highest H1 in our history. The strong performa...                                            \n",
       "19  device-led business, which helps clients plan ...                                            \n",
       "\n",
       "   ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2023228 DN000000003004320905.txt  \\\n",
       "0                St James's Place PLC (STJ LN Equity)                                            \n",
       "1                                          2023-02-28                                            \n",
       "2   Good morning, and welcome to our Full-Year Res...                                            \n",
       "3   undoubtedly another extraordinary year. After ...                                            \n",
       "4   geopolitical conditions across the globe quick...                                            \n",
       "5   as the year progressed. We had to contend with...                                            \n",
       "6   and the conict in Ukraine which combined to c...                                            \n",
       "7   And in the UK this was compounded by political...                                            \n",
       "8   environment 2022 marked the second-best year f...                                            \n",
       "9   Place history. I will recap on these ows then...                                            \n",
       "10  Financials before returning to provide an upda...                                            \n",
       "11  So turning to ows. Our gures frank the stren...                                            \n",
       "12  highlight the fantastic job our advisors do to...                                            \n",
       "13  want. During the year our advisors attracted G...                                            \n",
       "14  making 2022 our second-best year for new busin...                                            \n",
       "15  reasons why we've been able to achieve this de...                                            \n",
       "16  far the most important of these is our fully a...                                            \n",
       "17  making long-term nancial plans and long-term ...                                            \n",
       "18  Even though the environment deteriorate as the...                                            \n",
       "19  savers compounded our new business held up wel...                                            \n",
       "\n",
       "   ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2023727 DN000000003017788687.txt  \\\n",
       "0                St James's Place PLC (STJ LN Equity)                                            \n",
       "1                                          2023-07-27                                            \n",
       "2   Good morning, and welcome to our 2023 Half-Yea...                                            \n",
       "3   presentation has been pre-recorded, and we wil...                                            \n",
       "4   This morning's session will follow a familiar ...                                            \n",
       "5   of our new business gures and how we prepared...                                            \n",
       "6   regime. I will then hand over to Craig who wil...                                            \n",
       "7   results. We will then be back to me where I'll...                                            \n",
       "8   progress against our business priorities and h...                                            \n",
       "9                                                SJP.                                            \n",
       "10  So starting with new business. Most apparent i...                                            \n",
       "11  and savers has been challenging. Ination has ...                                            \n",
       "12  rising pressure on household budgets and savin...                                            \n",
       "13  has reacted by raising interest rates, and thi...                                            \n",
       "14                            savers and borrowers.                                              \n",
       "15  sluggish at best. While this paints a gloomy p...                                            \n",
       "16  are of course empathetic to the diculties hou...                                            \n",
       "17  remember that the need for advice isn't going ...                                            \n",
       "18  individuals realize the challenges to achievin...                                            \n",
       "19  Our advisors are facing into this by supportin...                                            \n",
       "\n",
       "   ST._JAMES_place/cleaned_St James_s Place PLC Earnings Call 2024228 DN000000003032811522.txt  \\\n",
       "0                St James's Place PLC (STJ LN Equity)                                            \n",
       "1                                          2024-02-28                                            \n",
       "2   Good morning, everyone. It's my pleasure to ta...                                            \n",
       "3   presentation as CEO of St. James's Place. We h...                                            \n",
       "4   include how we're dealing with two historical ...                                            \n",
       "5   throughout this presentation how fundamentally...                                            \n",
       "6   of this businesses. We continue to attract str...                                            \n",
       "7   management and deliver robust underlying nanc...                                            \n",
       "8                      challenging market conditions.                                            \n",
       "9   Digging into those headlines a little more, we...                                            \n",
       "10  billion in client funds. Our partnership conti...                                            \n",
       "11  investments amounting to GBP15.4 billion of gr...                                            \n",
       "12  our client retention rates have stayed strong ...                                            \n",
       "13  our relationships with clients and their cond...                                            \n",
       "14  these resulted in net inows of GBP5.1 billion...                                            \n",
       "15  performance driving gains of GBP14.7 billion, ...                                            \n",
       "16             grown by almost GBP20 billion in 2023.                                            \n",
       "17  To put that into context, after this business ...                                            \n",
       "18  funds under management to reach GBP20 billion,...                                            \n",
       "19  have today. In terms of underlying nancial pe...                                            \n",
       "\n",
       "    ...  \\\n",
       "0   ...   \n",
       "1   ...   \n",
       "2   ...   \n",
       "3   ...   \n",
       "4   ...   \n",
       "5   ...   \n",
       "6   ...   \n",
       "7   ...   \n",
       "8   ...   \n",
       "9   ...   \n",
       "10  ...   \n",
       "11  ...   \n",
       "12  ...   \n",
       "13  ...   \n",
       "14  ...   \n",
       "15  ...   \n",
       "16  ...   \n",
       "17  ...   \n",
       "18  ...   \n",
       "19  ...   \n",
       "\n",
       "   Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 20221019 DN000000002995149139.txt  \\\n",
       "0               Travelers Cos Inc/The (TRV US Equity)                                              \n",
       "1                                          2022-10-19                                              \n",
       "2   Good morning, ladies and gentlemen. Welcome to...                                              \n",
       "3   Teleconference for Travelers. We ask that you ...                                              \n",
       "4   of formal remarks at which time you will be gi...                                              \n",
       "5   answer session. As a reminder, this conference...                                              \n",
       "6                                               2022.                                              \n",
       "7   At this time, I would like to turn the confere...                                              \n",
       "8   Vice President of Investor Relations. Ms. Gold...                                              \n",
       "9   2022 results. We released our press release, ...                                              \n",
       "10  presentation earlier this morning. All of thes...                                              \n",
       "11  at travelers.com under the Investors section. ...                                              \n",
       "12  Chairman and CEO; Dan Frey, Chief Financial O...                                              \n",
       "13  presidents Greg Toczydlowski of Business Insur...                                              \n",
       "14  Insurance, and Michael Klein of Personal Insur...                                              \n",
       "15  results of our business and the current market...                                              \n",
       "16  webcast presentation as they go through prepar...                                              \n",
       "17                                    your questions.                                              \n",
       "18  Before I turn the call over to Alan, I'd like ...                                              \n",
       "19  note included at the end-of-the webcast presen...                                              \n",
       "\n",
       "   Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2022120 RT000000002973194137.txt  \\\n",
       "0               Travelers Cos Inc/The (TRV US Equity)                                             \n",
       "1                                          2022-01-20                                             \n",
       "2   Good morning, ladies and gentlemen, and welcom...                                             \n",
       "3   Teleconference for Travelers. We ask that you ...                                             \n",
       "4   of formal remarks. At which time, you will be ...                                             \n",
       "5   answer session. As a reminder, this conference...                                             \n",
       "6                                               2022.                                             \n",
       "7   At this time, I would like to turn the confere...                                             \n",
       "8   Vice President of Investor Relations. Ms.Golds...                                             \n",
       "9                                            20602454                                             \n",
       "10  Thank you. Good morning, and welcome to Travel...                                             \n",
       "11  quarter 2021 results. We released our press re...                                             \n",
       "12  webcast presentation earlier this morning. All...                                             \n",
       "13  website at travelers.com under the Investors s...                                             \n",
       "14  Speaking today will be Alan Schnitzer, Chairma...                                             \n",
       "15  Ocer; and our three-segment Presidents; Greg ...                                             \n",
       "16  Je Klenk of Bond & Specialty Insurance; and M...                                             \n",
       "17  They will discuss the nancial results of our ...                                             \n",
       "18  environment. They will refer to the webcast pr...                                             \n",
       "19  prepared remarks and then we will take questions.                                             \n",
       "\n",
       "   Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2022419 RT000000002979120029.txt  \\\n",
       "0               Travelers Cos Inc/The (TRV US Equity)                                             \n",
       "1                                          2022-04-19                                             \n",
       "2   Good morning, ladies and gentlemen. Welcome to...                                             \n",
       "3   Teleconference for Travelers. (Operator Instru...                                             \n",
       "4                   being recorded on April 19, 2022.                                             \n",
       "5   At this time, I would like to turn the confere...                                             \n",
       "6   Vice President of Investor Relations. Ms.Golds...                                             \n",
       "7   Thank you. Good morning, and welcome to Travel...                                             \n",
       "8   2022 results. We released our press release, ...                                             \n",
       "9       at travelers.com under the Investors section.                                             \n",
       "10  Speaking today will be Alan Schnitzer, Chairma...                                             \n",
       "11  Ocer; and our three segment Presidents, Greg ...                                             \n",
       "12  Je Klenk of Bond & Specialty Insurance, and M...                                             \n",
       "13  They will discuss the nancial results of our ...                                             \n",
       "14  environment. They will refer to the webcast pr...                                             \n",
       "15  prepared remarks, and then we will take questi...                                             \n",
       "16  Before I turn the call over, I'd like to draw ...                                             \n",
       "17  included at the end of the webcast presentatio...                                             \n",
       "18  forward-looking statements. The company cautio...                                             \n",
       "19  looking statement involves risks and uncertain...                                             \n",
       "\n",
       "   Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2022721 RT000000002987761887.txt  \\\n",
       "0               Travelers Cos Inc/The (TRV US Equity)                                             \n",
       "1                                          2022-07-21                                             \n",
       "2   Good morning, ladies and gentlemen. Welcome to...                                             \n",
       "3   Teleconference for Travelers. We ask that you ...                                             \n",
       "4   of formal remarks, at which time you will be g...                                             \n",
       "5   answer session. As a reminder, this conference...                                             \n",
       "6   At this time, I would like to turn the confere...                                             \n",
       "7   Vice President of Investor Relations. Ms.Golds...                                             \n",
       "8   quarter 2022 results. We released our press re...                                             \n",
       "9   webcast presentation earlier this morning. All...                                             \n",
       "10  website at travelers.com under the Investors s...                                             \n",
       "11  Speaking today will be Alan Schnitzer, Chairma...                                             \n",
       "12  three segment presidents, Greg Toczydlowski of...                                             \n",
       "13  Bond & Specialty Insurance; and Michael Klein ...                                             \n",
       "14  discuss nancial results of our business and t...                                             \n",
       "15  will refer to the webcast presentation as they...                                             \n",
       "16                  then we will take your questions.                                             \n",
       "17  Before I turn the call over to Alan, I would l...                                             \n",
       "18  explanatory note included at the end of the we...                                             \n",
       "19  today includes forward-looking statements. The...                                             \n",
       "\n",
       "   Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 20231018 DN000000003023919633.txt  \\\n",
       "0               Travelers Cos Inc/The (TRV US Equity)                                              \n",
       "1                                          2023-10-18                                              \n",
       "2   Good morning, ladies and gentlemen. Welcome to...                                              \n",
       "3   Teleconference for Travelers. We ask that you ...                                              \n",
       "4   of formal remarks, at which time, you will be ...                                              \n",
       "5   answer session. As a reminder, this conference...                                              \n",
       "6                                               2023.                                              \n",
       "7   At this time, I would like to turn the confere...                                              \n",
       "8   Vice President of Investor Relations. Ms. Gold...                                              \n",
       "9                                            20602454                                              \n",
       "10  Thank you. Good morning, and welcome to Travel...                                              \n",
       "11  2023 results. We released our press release, ...                                              \n",
       "12  sorry, and webcast presentation earlier this m...                                              \n",
       "13  found on our website at travelers.com under th...                                              \n",
       "14  Speaking today will be Alan Schnitzer, Chairma...                                              \n",
       "15  Ocer; and our three segment Presidents, Greg ...                                              \n",
       "16  Je Klenk of Bond & Specialty Insurance; and M...                                              \n",
       "17  They will discuss the nancial results of our ...                                              \n",
       "18  environment. They will refer to the webcast pr...                                              \n",
       "19  prepared remarks, and then we will take questi...                                              \n",
       "\n",
       "   Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2023124 RT000000003006445379.txt  \\\n",
       "0               Travelers Cos Inc/The (TRV US Equity)                                             \n",
       "1                                          2023-01-24                                             \n",
       "2   Good morning, ladies and gentlemen. Welcome to...                                             \n",
       "3   Teleconference for Travelers. We ask that you ...                                             \n",
       "4   of formal remarks, at which time you will be g...                                             \n",
       "5   answer session. As a reminder, this conference...                                             \n",
       "6   At this time, I would like to turn the confere...                                             \n",
       "7   Vice President of Investor Relations. Ms.Golds...                                             \n",
       "8   Thank you. Good morning, and welcome to Travel...                                             \n",
       "9   quarter 2022 results. We released our press re...                                             \n",
       "10  webcast presentation earlier this morning. All...                                             \n",
       "11  website at travelers.com under the Investors s...                                             \n",
       "12  Ocer; and our three segment Presidents Greg T...                                             \n",
       "13  Je Klenk of Bond & Specialty Insurance, and M...                                             \n",
       "14  They will discuss the nancial results of our ...                                             \n",
       "15  environment. They will refer to the webcast pr...                                             \n",
       "16  prepared remarks and then we will take your qu...                                             \n",
       "17  Before I turn the call over to Alan, I'd like ...                                             \n",
       "18  note included at the end of the webcast presen...                                             \n",
       "19  includes forward-looking statements. The compa...                                             \n",
       "\n",
       "   Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2023419 DN000000003009088712.txt  \\\n",
       "0               Travelers Cos Inc/The (TRV US Equity)                                             \n",
       "1                                          2023-04-19                                             \n",
       "2   Good morning, ladies and gentlemen. Welcome to...                                             \n",
       "3   Teleconference for Travelers. We ask that you ...                                             \n",
       "4   of formal remarks, at which time you will be g...                                             \n",
       "5   answer session. As a reminder, this conference...                                             \n",
       "6   At this time, I would like to turn the confere...                                             \n",
       "7   Vice President of Investor Relations. Ms. Gold...                                             \n",
       "8                                            20602454                                             \n",
       "9   Thank you. Good morning and welcome to Travele...                                             \n",
       "10  2023 results. We released our press release, ...                                             \n",
       "11  presentation earlier this morning. All of thes...                                             \n",
       "12      at travelers.com under the Investors section.                                             \n",
       "13  Speaking today will be Alan Schnitzer, Chairma...                                             \n",
       "14  Ocer; and our three segment presidents, Greg ...                                             \n",
       "15  Je Klenk of Bond & Specialty Insurance; and M...                                             \n",
       "16  They will discuss the nancial results of our ...                                             \n",
       "17  environment. They will refer to the webcast pr...                                             \n",
       "18  prepared remarks, and then we will take questi...                                             \n",
       "19  Before I turn the call over to Alan, I'd like ...                                             \n",
       "\n",
       "   Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2023720 DN000000003017167658.txt  \\\n",
       "0               Travelers Cos Inc/The (TRV US Equity)                                             \n",
       "1                                          2023-07-20                                             \n",
       "2   Good morning, ladies and gentlemen. Welcome to...                                             \n",
       "3   Teleconference for Travelers. We ask that you ...                                             \n",
       "4   of the formal remarks, at which time we will b...                                             \n",
       "5   and-answer session. As a reminder, this confer...                                             \n",
       "6                                               2023.                                             \n",
       "7   At this time. I would like to turn the confere...                                             \n",
       "8   Vice-President of Investor Relations. Ms. Gold...                                             \n",
       "9   Thank you. Good morning, and welcome to Travel...                                             \n",
       "10  quarter 2023 results. We released our press re...                                             \n",
       "11  website at travelers.com under the Investors s...                                             \n",
       "12  Speaking today will be Alan Schnitzer, Chairma...                                             \n",
       "13  three segment Presidents Greg Toczydlowski of ...                                             \n",
       "14  Bond & Specialty Insurance; and Michael Klein ...                                             \n",
       "15  discuss the nancial results of our business a...                                             \n",
       "16  They will refer to the webcast presentation as...                                             \n",
       "17                       then we will take questions.                                             \n",
       "18  Before I turn the call over to Alan, I'd like ...                                             \n",
       "19  note included at the end of the webcast presen...                                             \n",
       "\n",
       "   Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2024119 RT000000003030171756.txt  \\\n",
       "0               Travelers Cos Inc/The (TRV US Equity)                                             \n",
       "1                                          2024-01-19                                             \n",
       "2   Good morning, ladies and gentlemen. Welcome to...                                             \n",
       "3   Teleconference for Travelers. We ask that you ...                                             \n",
       "4   of formal remarks, at which time you will be g...                                             \n",
       "5   answer session. As a reminder, this conference...                                             \n",
       "6                                               2024.                                             \n",
       "7   At this time, I would like to turn the confere...                                             \n",
       "8   Vice President of Investor Relations. Ms.Golds...                                             \n",
       "9   quarter 2023 results. We released our press re...                                             \n",
       "10  webcast presentation earlier this morning. All...                                             \n",
       "11  website at travelers.com under the Investors s...                                             \n",
       "12  Speaking today will be Alan Schnitzer, Chairma...                                             \n",
       "13  Ocer; and our three segment presidents, Greg ...                                             \n",
       "14  Je Klenk of Bond and Specialty Insurance; and...                                             \n",
       "15  They will discuss the nancial results of our ...                                             \n",
       "16  environment. They will refer to the webcast pr...                                             \n",
       "17  prepared remarks, and then we will take your q...                                             \n",
       "18  Before I turn the call over to Alan, I'd like ...                                             \n",
       "19  note included at the end of the webcast presen...                                             \n",
       "\n",
       "   Traveler_Cos_TRV/cleaned_Travelers Cos IncThe Earnings Call 2024417 DN000000003036347706.txt  \n",
       "0               Travelers Cos Inc/The (TRV US Equity)                                            \n",
       "1                                          2024-04-17                                            \n",
       "2   Good morning, ladies and gentlemen. Welcome to...                                            \n",
       "3   Teleconference for Travelers. We ask that you ...                                            \n",
       "4   of formal remarks, at which time you will be g...                                            \n",
       "5   answer session. As a reminder, this conference...                                            \n",
       "6   At this time, I would like to turn the confere...                                            \n",
       "7   Vice President of Investor Relations. Ms. Gold...                                            \n",
       "8   Thank you. Good morning, and welcome to Travel...                                            \n",
       "9   2024 results. We released our press release, ...                                            \n",
       "10      at travelers.com under the Investors section.                                            \n",
       "11  Speaking today will be Alan Schnitzer, Chairma...                                            \n",
       "12  Ocer; and our three Segment Presidents, Greg ...                                            \n",
       "13  Je Klenk of Bond & Specialty Insurance; and M...                                            \n",
       "14  They will discuss the nancial results of our ...                                            \n",
       "15  environment. They will refer to the webcast pr...                                            \n",
       "16  prepared remarks and then we will take your qu...                                            \n",
       "17  Before I turn the call over to Alan, I'd like ...                                            \n",
       "18  note included at the end of the webcast presen...                                            \n",
       "19  includes forward-looking statements. The compa...                                            \n",
       "\n",
       "[20 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory containing the cleaned text files\n",
    "input_dir = \"Cleaned Earnings Calls Texts\"\n",
    "\n",
    "# Initialise a dictionary to hold the text data\n",
    "text_data = {}\n",
    "\n",
    "# Check if the input directory exists and is not empty\n",
    "if not os.path.exists(input_dir) or not os.listdir(input_dir):\n",
    "    print(f\"Directory {input_dir} does not exist or is empty.\")\n",
    "else:\n",
    "    # Iterate over each company folder in the directory\n",
    "    for company_folder in os.listdir(input_dir):\n",
    "        company_path = os.path.join(input_dir, company_folder)\n",
    "        if os.path.isdir(company_path):\n",
    "            # Iterate over each text file in the company folder\n",
    "            for filename in os.listdir(company_path):\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(company_path, filename)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        text = file.read()\n",
    "                        lines = text.split('\\n')\n",
    "                        # Remove lines that begin and end with \"Presentation\" or \"Questions And Answers\"\n",
    "                        lines = [line for line in lines if not (line.strip().startswith(\"Presentation\") and line.strip().endswith(\"Presentation\"))]\n",
    "                        lines = [line for line in lines if not (line.strip().startswith(\"Questions And Answers\") and line.strip().endswith(\"Questions And Answers\"))]\n",
    "                        text_data[f\"{company_folder}/{filename}\"] = lines\n",
    "\n",
    "    # Check if text_data is empty\n",
    "    if not text_data:\n",
    "        print(f\"No valid text files found in {input_dir}.\")\n",
    "    else:\n",
    "        # Determine the maximum number of lines in the files\n",
    "        max_lines = max(len(lines) for lines in text_data.values())\n",
    "\n",
    "        # Create a DataFrame with the file names as columns\n",
    "        df = pd.DataFrame({filename: lines + [''] * (max_lines - len(lines)) for filename, lines in text_data.items()})\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1092"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below compiles the data into a DataFrame with columns for the text line, file name, date, and company name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should we get started? So good morning, everyo...</td>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>presentation. Adopting our usual format at the...</td>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hand over to Craig to run through the nancial...</td>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>developments and outlook. We'll follow this wi...</td>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24177</th>\n",
       "      <td>Thank you. I will turn the call to Ms. Goldste...</td>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24178</th>\n",
       "      <td>follow-up please feel free to reach out direct...</td>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24179</th>\n",
       "      <td>day.</td>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24180</th>\n",
       "      <td>This concludes today's conference call. We tha...</td>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24181</th>\n",
       "      <td>disconnect your lines.</td>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24182 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    line  \\\n",
       "0      Should we get started? So good morning, everyo...   \n",
       "1      presentation. Adopting our usual format at the...   \n",
       "2      hand over to Craig to run through the nancial...   \n",
       "3      developments and outlook. We'll follow this wi...   \n",
       "4      There are also a number of my executive team a...   \n",
       "...                                                  ...   \n",
       "24177  Thank you. I will turn the call to Ms. Goldste...   \n",
       "24178  follow-up please feel free to reach out direct...   \n",
       "24179                                               day.   \n",
       "24180  This concludes today's conference call. We tha...   \n",
       "24181                             disconnect your lines.   \n",
       "\n",
       "                                               file_name        date  \\\n",
       "0      cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "1      cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "2      cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "3      cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "4      cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "...                                                  ...         ...   \n",
       "24177  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "24178  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "24179  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "24180  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "24181  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "\n",
       "                                company_name  \n",
       "0       St James's Place PLC (STJ LN Equity)  \n",
       "1       St James's Place PLC (STJ LN Equity)  \n",
       "2       St James's Place PLC (STJ LN Equity)  \n",
       "3       St James's Place PLC (STJ LN Equity)  \n",
       "4       St James's Place PLC (STJ LN Equity)  \n",
       "...                                      ...  \n",
       "24177  Travelers Cos Inc/The (TRV US Equity)  \n",
       "24178  Travelers Cos Inc/The (TRV US Equity)  \n",
       "24179  Travelers Cos Inc/The (TRV US Equity)  \n",
       "24180  Travelers Cos Inc/The (TRV US Equity)  \n",
       "24181  Travelers Cos Inc/The (TRV US Equity)  \n",
       "\n",
       "[24182 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory containing the cleaned text files\n",
    "input_dir = \"Cleaned Earnings Calls Texts\"\n",
    "\n",
    "# Initialise a list to hold the text data\n",
    "text_data = []\n",
    "\n",
    "# Iterate over each company folder in the directory\n",
    "for company_folder in os.listdir(input_dir):\n",
    "    company_path = os.path.join(input_dir, company_folder)\n",
    "    if os.path.isdir(company_path):\n",
    "        # Iterate over each text file in the company folder\n",
    "        for filename in os.listdir(company_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(company_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    lines = text.split('\\n')\n",
    "                    # Extract company name and date\n",
    "                    if len(lines) > 1:\n",
    "                        company_name = lines[0]\n",
    "                        date = lines[1]\n",
    "                        # Remove the first two lines (company name and date)\n",
    "                        lines = lines[2:]\n",
    "                        # Remove lines that begin and end with \"Presentation\" or \"Questions And Answers\"\n",
    "                        lines = [line for line in lines if not (line.strip().startswith(\"Presentation\") and line.strip().endswith(\"Presentation\"))]\n",
    "                        lines = [line for line in lines if not (line.strip().startswith(\"Questions And Answers\") and line.strip().endswith(\"Questions And Answers\"))]\n",
    "                        for line in lines:\n",
    "                            text_data.append([line, filename, date, company_name])\n",
    "\n",
    "# Create a DataFrame with the specified structure\n",
    "horizontal_df = pd.DataFrame(text_data, columns=['line', 'file_name', 'date', 'company_name'])\n",
    "\n",
    "# Print the new DataFrame\n",
    "horizontal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24182"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(horizontal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Paragraph-Based DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code combines lines into paragraphs. Each paragraph is then stored in a DataFrame along with its corresponding file name, date and company name. Duplicate paragraphs are identified and removed, and the DataFrame is reset to ensure continuous indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>company_name</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Should we get started? So good morning, everyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Please do look them up over coee at the end.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>So the rst six months. It's fair to say that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>New gross inows for the six months was GBP 7....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Hi, good morning and thanks for squeezing me i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4526</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Thank you. It helps. Always much appreciated.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4527</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Thank you. I will turn the call to Ms. Goldste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>This concludes today's conference call. We tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4529 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_name        date  \\\n",
       "0     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "1     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "2     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "3     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "4     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "...                                                 ...         ...   \n",
       "4524  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4525  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4526  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4527  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4528  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "\n",
       "                               company_name  \\\n",
       "0      St James's Place PLC (STJ LN Equity)   \n",
       "1      St James's Place PLC (STJ LN Equity)   \n",
       "2      St James's Place PLC (STJ LN Equity)   \n",
       "3      St James's Place PLC (STJ LN Equity)   \n",
       "4      St James's Place PLC (STJ LN Equity)   \n",
       "...                                     ...   \n",
       "4524  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4525  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4526  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4527  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4528  Travelers Cos Inc/The (TRV US Equity)   \n",
       "\n",
       "                                              paragraph  \n",
       "0     Should we get started? So good morning, everyo...  \n",
       "1     There are also a number of my executive team a...  \n",
       "2         Please do look them up over coee at the end.  \n",
       "3     So the rst six months. It's fair to say that ...  \n",
       "4     New gross inows for the six months was GBP 7....  \n",
       "...                                                 ...  \n",
       "4524  Hi, good morning and thanks for squeezing me i...  \n",
       "4525  I did notice that the renewal rate in home did...  \n",
       "4526      Thank you. It helps. Always much appreciated.  \n",
       "4527  Thank you. I will turn the call to Ms. Goldste...  \n",
       "4528  This concludes today's conference call. We tha...  \n",
       "\n",
       "[4529 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory containing the cleaned text files\n",
    "input_dir = \"Cleaned Earnings Calls Texts\"\n",
    "\n",
    "# Initialize a list to hold the text data\n",
    "text_data = []\n",
    "\n",
    "# Function to determine if a line ends with a period and the next line starts with a capital letter\n",
    "def is_end_of_paragraph(line, next_line):\n",
    "    return bool(re.search(r'\\.\\s*$', line)) and next_line and next_line[0].isupper()\n",
    "\n",
    "# Iterate over each company folder in the directory\n",
    "for company_folder in os.listdir(input_dir):\n",
    "    company_path = os.path.join(input_dir, company_folder)\n",
    "    if os.path.isdir(company_path):\n",
    "        # Iterate over each text file in the company folder\n",
    "        for filename in os.listdir(company_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(company_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    lines = text.split('\\n')\n",
    "                    # Remove lines that begin and end with \"Presentation\" or \"Questions And Answers\"\n",
    "                    lines = [line for line in lines if not (line.strip().startswith(\"Presentation\") and line.strip().endswith(\"Presentation\"))]\n",
    "                    lines = [line for line in lines if not (line.strip().startswith(\"Questions And Answers\") and line.strip().endswith(\"Questions And Answers\"))]\n",
    "\n",
    "                    if len(lines) > 1:\n",
    "                        company_name = lines[0]\n",
    "                        date = lines[1]\n",
    "                        paragraph = \"\"\n",
    "                        for i in range(2, len(lines)):\n",
    "                            line = lines[i]\n",
    "                            next_line = lines[i + 1] if i + 1 < len(lines) else \"\"\n",
    "                            if is_end_of_paragraph(line, next_line):\n",
    "                                paragraph += line + \" \"\n",
    "                                text_data.append([filename, date, company_name, paragraph.strip()])\n",
    "                                paragraph = \"\"\n",
    "                            else:\n",
    "                                paragraph += line + \" \"\n",
    "                        \n",
    "                        # Add the last paragraph if there is no trailing empty line\n",
    "                        if paragraph:\n",
    "                            text_data.append([filename, date, company_name, paragraph.strip()])\n",
    "\n",
    "# Create a DataFrame with the specified structure\n",
    "paragraphs_df = pd.DataFrame(text_data, columns=['file_name', 'date', 'company_name', 'paragraph'])\n",
    "\n",
    "# Print the new DataFrame\n",
    "paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 4529\n",
      "true_false\n",
      "False    4211\n",
      "True      318\n",
      "Name: count, dtype: int64\n",
      "After: 4211\n"
     ]
    }
   ],
   "source": [
    "# Display the length of the DataFrame before removing duplicates\n",
    "print(\"Before:\", len(paragraphs_df))\n",
    "\n",
    "# Create a copy of the DataFrame to check for duplicates\n",
    "check_dup = paragraphs_df.copy()\n",
    "\n",
    "# Check for duplicate paragraphs\n",
    "check_dup['true_false'] = paragraphs_df.duplicated(subset=['paragraph'])\n",
    "\n",
    "# Display the count of duplicate and unique paragraphs\n",
    "print(check_dup['true_false'].value_counts())\n",
    "\n",
    "# Keep only one row for each unique paragraph\n",
    "paragraphs_df = paragraphs_df.drop_duplicates(subset=['paragraph'])\n",
    "\n",
    "# Reset the index to make sure it's continuous\n",
    "paragraphs_df = paragraphs_df.reset_index(drop=True)\n",
    "\n",
    "# Display the length of the DataFrame after removing duplicates\n",
    "print(\"After:\", len(paragraphs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name\n",
      "cleaned_St James_s Place PLC Earnings Call 2019731 SD000000002886567163.txt     151\n",
      "cleaned_St James_s Place PLC Earnings Call 2020227 DN000000002799172133.txt     140\n",
      "cleaned_St James_s Place PLC Earnings Call 2020728 DN000000002875532448.txt     159\n",
      "cleaned_St James_s Place PLC Earnings Call 2021225 RT000000002951492856.txt     141\n",
      "cleaned_St James_s Place PLC Earnings Call 2021728 RT000000002961195990.txt     170\n",
      "cleaned_St James_s Place PLC Earnings Call 2022224 DN000000002974539999.txt     144\n",
      "cleaned_St James_s Place PLC Earnings Call 2022728 DN000000002988128310.txt     130\n",
      "cleaned_St James_s Place PLC Earnings Call 2023228 DN000000003004320905.txt     142\n",
      "cleaned_St James_s Place PLC Earnings Call 2023727 DN000000003017788687.txt     135\n",
      "cleaned_St James_s Place PLC Earnings Call 2024228 DN000000003032811522.txt     193\n",
      "cleaned_Travelers Cos IncThe Earnings Call 20191022 DN000000002732661665.txt    188\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2019418 DN000000002625973556.txt     119\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2019723 RT000000002897838852.txt     119\n",
      "cleaned_Travelers Cos IncThe Earnings Call 20201020 DN000000002919519869.txt    140\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2020123 DN000000002782019895.txt     145\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2020421 RT000000002827435712.txt     144\n",
      "cleaned_Travelers Cos IncThe Earnings Call 20211019 DN000000002966110191.txt    129\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2021121 RT000000002948798175.txt     151\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2021420 DN000000002958474957.txt     130\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2021720 RT000000002960746057.txt     112\n",
      "cleaned_Travelers Cos IncThe Earnings Call 20221019 DN000000002995149139.txt    148\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2022120 RT000000002973194137.txt     152\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2022419 RT000000002979120029.txt     143\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2022721 RT000000002987761887.txt     120\n",
      "cleaned_Travelers Cos IncThe Earnings Call 20231018 DN000000003023919633.txt    111\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2023124 RT000000003006445379.txt     127\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2023419 DN000000003009088712.txt     148\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2023720 DN000000003017167658.txt     115\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2024119 RT000000003030171756.txt     161\n",
      "cleaned_Travelers Cos IncThe Earnings Call 2024417 DN000000003036347706.txt     104\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "paragraph_counts = paragraphs_df.groupby('file_name').size()\n",
    "print(paragraph_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think of that as a credit business. But maybe things have changed.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a random paragraph\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    random_paragraph = paragraphs_df['paragraph'].sample(n=1).iloc[0]\n",
    "random_paragraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Lemmatising "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script utilises spaCy to process and clean the text data. Initially, it defines a function to clean tokens by removing whitespace, stop words, numbers and punctuation while lemmatizing the text. Non-English paragraphs are filtered out, and the DataFrame is reset for continuous indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>company_name</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>paragraph_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Should we get started? So good morning, everyo...</td>\n",
       "      <td>Should we get started? So good morning everyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Please do look them up over coee at the end.</td>\n",
       "      <td>Please do look them up over coee at the end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>So the rst six months. It's fair to say that ...</td>\n",
       "      <td>So the rst six months It's fair to say that w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>New gross inows for the six months was GBP 7....</td>\n",
       "      <td>New gross inows for the six months was GBP  b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Hi, good morning and thanks for squeezing me i...</td>\n",
       "      <td>Hi good morning and thanks for squeezing me in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Thank you. It helps. Always much appreciated.</td>\n",
       "      <td>Thank you It helps Always much appreciated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4209</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Thank you. I will turn the call to Ms. Goldste...</td>\n",
       "      <td>Thank you I will turn the call to Ms Goldstein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4210</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>This concludes today's conference call. We tha...</td>\n",
       "      <td>This concludes today's conference call We than...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4211 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_name        date  \\\n",
       "0     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "1     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "2     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "3     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "4     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "...                                                 ...         ...   \n",
       "4206  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4207  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4208  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4209  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4210  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "\n",
       "                               company_name  \\\n",
       "0      St James's Place PLC (STJ LN Equity)   \n",
       "1      St James's Place PLC (STJ LN Equity)   \n",
       "2      St James's Place PLC (STJ LN Equity)   \n",
       "3      St James's Place PLC (STJ LN Equity)   \n",
       "4      St James's Place PLC (STJ LN Equity)   \n",
       "...                                     ...   \n",
       "4206  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4207  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4208  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4209  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4210  Travelers Cos Inc/The (TRV US Equity)   \n",
       "\n",
       "                                              paragraph  \\\n",
       "0     Should we get started? So good morning, everyo...   \n",
       "1     There are also a number of my executive team a...   \n",
       "2         Please do look them up over coee at the end.   \n",
       "3     So the rst six months. It's fair to say that ...   \n",
       "4     New gross inows for the six months was GBP 7....   \n",
       "...                                                 ...   \n",
       "4206  Hi, good morning and thanks for squeezing me i...   \n",
       "4207  I did notice that the renewal rate in home did...   \n",
       "4208      Thank you. It helps. Always much appreciated.   \n",
       "4209  Thank you. I will turn the call to Ms. Goldste...   \n",
       "4210  This concludes today's conference call. We tha...   \n",
       "\n",
       "                                        paragraph_clean  \n",
       "0     Should we get started? So good morning everyon...  \n",
       "1     There are also a number of my executive team a...  \n",
       "2          Please do look them up over coee at the end  \n",
       "3     So the rst six months It's fair to say that w...  \n",
       "4     New gross inows for the six months was GBP  b...  \n",
       "...                                                 ...  \n",
       "4206  Hi good morning and thanks for squeezing me in...  \n",
       "4207  I did notice that the renewal rate in home did...  \n",
       "4208         Thank you It helps Always much appreciated  \n",
       "4209  Thank you I will turn the call to Ms Goldstein...  \n",
       "4210  This concludes today's conference call We than...  \n",
       "\n",
       "[4211 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to clean tokens in the text\n",
    "def clean_tokens(text_series):\n",
    "    # Convert text_series df to list\n",
    "    text_list = text_series.to_list()\n",
    "\n",
    "    # Remove whitespaces and trailing spaces\n",
    "    def remove_whitespace(text):\n",
    "        pattern = re.compile(r'\\s+') \n",
    "        without_whitespace = re.sub(pattern, ' ', text)\n",
    "        text = without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    text_list = list(map(lambda x: remove_whitespace(x), text_list))\n",
    "\n",
    "    # Apply NLP pipeline to remove stop words, numbers, and lemmatize the words\n",
    "    tokens = []\n",
    "    for text in tqdm(text_list): # or tqdm.tqdm\n",
    "        tmp_tokens = [\n",
    "            token.lemma_\n",
    "            for token in nlp(text)\n",
    "            if not token.is_stop \n",
    "            and not token.like_num\n",
    "            and not token.is_punct\n",
    "            and token.is_alpha\n",
    "        ]\n",
    "        tokens.append(tmp_tokens)\n",
    "    return tokens \n",
    "\n",
    "# Function to post-process the DataFrame\n",
    "def post_process(df):\n",
    "    # Create a new column 'paragraph_clean'\n",
    "    df['paragraph_clean'] = df['paragraph']\n",
    "    \n",
    "    # Remove unwanted characters and numeric values\n",
    "    df['paragraph_clean'] = df['paragraph_clean'].str.replace(',', '', regex=False)\n",
    "    df['paragraph_clean'] = df['paragraph_clean'].str.replace('.', '', regex=False)\n",
    "    df['paragraph_clean'] = df['paragraph_clean'].str.replace('(', '', regex=False)\n",
    "    df['paragraph_clean'] = df['paragraph_clean'].str.replace(')', '', regex=False)\n",
    "    df['paragraph_clean'] = df['paragraph_clean'].str.replace(r'\\d+\\.\\d+', '', regex=True)\n",
    "    df['paragraph_clean'] = df['paragraph_clean'].str.replace('\\d+', '', regex=True)\n",
    "    df['paragraph_clean'] = df['paragraph_clean'].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the post_process function to paragraphs_df\n",
    "paragraphs_df = post_process(paragraphs_df)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(paragraphs_df['file_name'].nunique())\n",
    "paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>company_name</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>paragraph_clean</th>\n",
       "      <th>paragraph_noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Should we get started? So good morning, everyo...</td>\n",
       "      <td>Should we get started? So good morning everyon...</td>\n",
       "      <td>morning result presentation format year fund ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "      <td>number team non - exec morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Please do look them up over coee at the end.</td>\n",
       "      <td>Please do look them up over coee at the end</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>So the rst six months. It's fair to say that ...</td>\n",
       "      <td>So the rst six months It's fair to say that w...</td>\n",
       "      <td>month period uncertainty environment trade rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>New gross inows for the six months was GBP 7....</td>\n",
       "      <td>New gross inows for the six months was GBP  b...</td>\n",
       "      <td>gross inow month % half ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Hi, good morning and thanks for squeezing me i...</td>\n",
       "      <td>Hi good morning and thanks for squeezing me in...</td>\n",
       "      <td>morning thank couple line question renewal pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>rate home bit anomaly drop property year fact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Thank you. It helps. Always much appreciated.</td>\n",
       "      <td>Thank you It helps Always much appreciated</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4209</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Thank you. I will turn the call to Ms. Goldste...</td>\n",
       "      <td>Thank you I will turn the call to Ms Goldstein...</td>\n",
       "      <td>call closing remark follow up day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4210</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>This concludes today's conference call. We tha...</td>\n",
       "      <td>This concludes today's conference call We than...</td>\n",
       "      <td>today conference call line</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4211 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_name        date  \\\n",
       "0     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "1     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "2     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "3     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "4     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "...                                                 ...         ...   \n",
       "4206  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4207  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4208  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4209  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4210  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "\n",
       "                               company_name  \\\n",
       "0      St James's Place PLC (STJ LN Equity)   \n",
       "1      St James's Place PLC (STJ LN Equity)   \n",
       "2      St James's Place PLC (STJ LN Equity)   \n",
       "3      St James's Place PLC (STJ LN Equity)   \n",
       "4      St James's Place PLC (STJ LN Equity)   \n",
       "...                                     ...   \n",
       "4206  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4207  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4208  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4209  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4210  Travelers Cos Inc/The (TRV US Equity)   \n",
       "\n",
       "                                              paragraph  \\\n",
       "0     Should we get started? So good morning, everyo...   \n",
       "1     There are also a number of my executive team a...   \n",
       "2         Please do look them up over coee at the end.   \n",
       "3     So the rst six months. It's fair to say that ...   \n",
       "4     New gross inows for the six months was GBP 7....   \n",
       "...                                                 ...   \n",
       "4206  Hi, good morning and thanks for squeezing me i...   \n",
       "4207  I did notice that the renewal rate in home did...   \n",
       "4208      Thank you. It helps. Always much appreciated.   \n",
       "4209  Thank you. I will turn the call to Ms. Goldste...   \n",
       "4210  This concludes today's conference call. We tha...   \n",
       "\n",
       "                                        paragraph_clean  \\\n",
       "0     Should we get started? So good morning everyon...   \n",
       "1     There are also a number of my executive team a...   \n",
       "2          Please do look them up over coee at the end   \n",
       "3     So the rst six months It's fair to say that w...   \n",
       "4     New gross inows for the six months was GBP  b...   \n",
       "...                                                 ...   \n",
       "4206  Hi good morning and thanks for squeezing me in...   \n",
       "4207  I did notice that the renewal rate in home did...   \n",
       "4208         Thank you It helps Always much appreciated   \n",
       "4209  Thank you I will turn the call to Ms Goldstein...   \n",
       "4210  This concludes today's conference call We than...   \n",
       "\n",
       "                                         paragraph_noun  \n",
       "0     morning result presentation format year fund ...  \n",
       "1                        number team non - exec morning  \n",
       "2                                                   end  \n",
       "3     month period uncertainty environment trade rel...  \n",
       "4                          gross inow month % half ow  \n",
       "...                                                 ...  \n",
       "4206  morning thank couple line question renewal pre...  \n",
       "4207  rate home bit anomaly drop property year fact ...  \n",
       "4208                                                     \n",
       "4209                  call closing remark follow up day  \n",
       "4210                         today conference call line  \n",
       "\n",
       "[4211 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Lemmatization function\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\"]):\n",
    "    doc = nlp(texts)\n",
    "    new_text = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in allowed_postags:\n",
    "            new_text.append(token.lemma_)\n",
    "    final = \" \".join(new_text)\n",
    "    return final\n",
    "\n",
    "# Apply lemmatization to the 'paragraph_clean' column\n",
    "paragraphs_df['paragraph_noun'] = paragraphs_df['paragraph_clean'].apply(lemmatization)\n",
    "paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'paragraph_noun' if it is not in English\n",
    "paragraphs_df = paragraphs_df[paragraphs_df['paragraph_noun'].str.contains('[a-zA-Z]')]\n",
    "paragraphs_df = paragraphs_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenising and Analysing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script begins by converting text to lowercase and removing any extra whitespace. Following this, the script calculates the counts of words, characters, and sentences, excluding rows with fewer than three sentences. Additionally, overly general words are dropped from the tokens.\n",
    "\n",
    "Next, the script analyses word frequencies for each company and removes the 50 most frequent words. It also filters out tokens with fewer than two characters. The result is a final cleaned and tokenised DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def clean_tokens_noun(text_series):\n",
    "    # Step 1: Convert text_series df to list\n",
    "    text_list = text_series.to_list()\n",
    "\n",
    "    # Step 2: Change the list to lower case\n",
    "    text_list = list(map(lambda x: x.lower(), text_list))\n",
    "\n",
    "    # Step 3: Remove whitespaces and trailing spaces\n",
    "    def remove_whitespace(text):\n",
    "        pattern = re.compile(r'\\s+')\n",
    "        Without_whitespace = re.sub(pattern, ' ', text)\n",
    "        text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    text_list = list(map(lambda x: remove_whitespace(x), text_list))\n",
    "    \n",
    "    # Expand the list of stopwords\n",
    "    gist_file = open(\"gist_stopwords.txt\", \"r\")\n",
    "    try:\n",
    "        content = gist_file.read()\n",
    "        stopwords = content.split(\",\")\n",
    "    finally:\n",
    "        gist_file.close()\n",
    "\n",
    "    stopwords = [i.replace('\"', \"\").strip() for i in stopwords]\n",
    "    # Add the stopwords to the list of stopwords\n",
    "    for i in stopwords:\n",
    "        nlp.Defaults.stop_words.add(i)\n",
    "\n",
    "    # Create column for cleaned text_list\n",
    "    tokens, tmp_tokens = [], []\n",
    "    for text in tqdm(text_list):\n",
    "        tmp_tokens = [\n",
    "            token.lemma_\n",
    "            for token in nlp(text)\n",
    "            if not token.is_stop\n",
    "            and not token.like_num\n",
    "            and not token.is_punct\n",
    "            and token.is_alpha\n",
    "        ]\n",
    "        tokens.append(tmp_tokens)\n",
    "        tmp_tokens = []\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4102/4102 [00:15<00:00, 258.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>company_name</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>paragraph_clean</th>\n",
       "      <th>paragraph_noun</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Should we get started? So good morning, everyo...</td>\n",
       "      <td>Should we get started? So good morning everyon...</td>\n",
       "      <td>morning result presentation format year fund ...</td>\n",
       "      <td>[morning, result, presentation, format, year, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "      <td>number team non - exec morning</td>\n",
       "      <td>[number, team, exec, morning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Please do look them up over coee at the end.</td>\n",
       "      <td>Please do look them up over coee at the end</td>\n",
       "      <td>end</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>So the rst six months. It's fair to say that ...</td>\n",
       "      <td>So the rst six months It's fair to say that w...</td>\n",
       "      <td>month period uncertainty environment trade rel...</td>\n",
       "      <td>[month, period, uncertainty, environment, trad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>New gross inows for the six months was GBP 7....</td>\n",
       "      <td>New gross inows for the six months was GBP  b...</td>\n",
       "      <td>gross inow month % half ow</td>\n",
       "      <td>[gross, inow, month, half, ow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>We have time for one more question. It will --...</td>\n",
       "      <td>We have time for one more question It will -- ...</td>\n",
       "      <td>time question line line</td>\n",
       "      <td>[time, question]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Hi, good morning and thanks for squeezing me i...</td>\n",
       "      <td>Hi good morning and thanks for squeezing me in...</td>\n",
       "      <td>morning thank couple line question renewal pre...</td>\n",
       "      <td>[morning, couple, question, renewal, premium, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>rate home bit anomaly drop property year fact ...</td>\n",
       "      <td>[rate, bit, anomaly, drop, property, year, fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Thank you. I will turn the call to Ms. Goldste...</td>\n",
       "      <td>Thank you I will turn the call to Ms Goldstein...</td>\n",
       "      <td>call closing remark follow up day</td>\n",
       "      <td>[closing, remark, follow, day]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4101</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>This concludes today's conference call. We tha...</td>\n",
       "      <td>This concludes today's conference call We than...</td>\n",
       "      <td>today conference call line</td>\n",
       "      <td>[today, conference]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4102 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_name        date  \\\n",
       "0     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "1     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "2     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "3     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "4     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "...                                                 ...         ...   \n",
       "4097  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4098  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4099  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4100  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4101  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "\n",
       "                               company_name  \\\n",
       "0      St James's Place PLC (STJ LN Equity)   \n",
       "1      St James's Place PLC (STJ LN Equity)   \n",
       "2      St James's Place PLC (STJ LN Equity)   \n",
       "3      St James's Place PLC (STJ LN Equity)   \n",
       "4      St James's Place PLC (STJ LN Equity)   \n",
       "...                                     ...   \n",
       "4097  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4098  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4099  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4100  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4101  Travelers Cos Inc/The (TRV US Equity)   \n",
       "\n",
       "                                              paragraph  \\\n",
       "0     Should we get started? So good morning, everyo...   \n",
       "1     There are also a number of my executive team a...   \n",
       "2         Please do look them up over coee at the end.   \n",
       "3     So the rst six months. It's fair to say that ...   \n",
       "4     New gross inows for the six months was GBP 7....   \n",
       "...                                                 ...   \n",
       "4097  We have time for one more question. It will --...   \n",
       "4098  Hi, good morning and thanks for squeezing me i...   \n",
       "4099  I did notice that the renewal rate in home did...   \n",
       "4100  Thank you. I will turn the call to Ms. Goldste...   \n",
       "4101  This concludes today's conference call. We tha...   \n",
       "\n",
       "                                        paragraph_clean  \\\n",
       "0     Should we get started? So good morning everyon...   \n",
       "1     There are also a number of my executive team a...   \n",
       "2          Please do look them up over coee at the end   \n",
       "3     So the rst six months It's fair to say that w...   \n",
       "4     New gross inows for the six months was GBP  b...   \n",
       "...                                                 ...   \n",
       "4097  We have time for one more question It will -- ...   \n",
       "4098  Hi good morning and thanks for squeezing me in...   \n",
       "4099  I did notice that the renewal rate in home did...   \n",
       "4100  Thank you I will turn the call to Ms Goldstein...   \n",
       "4101  This concludes today's conference call We than...   \n",
       "\n",
       "                                         paragraph_noun  \\\n",
       "0     morning result presentation format year fund ...   \n",
       "1                        number team non - exec morning   \n",
       "2                                                   end   \n",
       "3     month period uncertainty environment trade rel...   \n",
       "4                          gross inow month % half ow   \n",
       "...                                                 ...   \n",
       "4097                            time question line line   \n",
       "4098  morning thank couple line question renewal pre...   \n",
       "4099  rate home bit anomaly drop property year fact ...   \n",
       "4100                  call closing remark follow up day   \n",
       "4101                         today conference call line   \n",
       "\n",
       "                                                  token  \n",
       "0     [morning, result, presentation, format, year, ...  \n",
       "1                         [number, team, exec, morning]  \n",
       "2                                                    []  \n",
       "3     [month, period, uncertainty, environment, trad...  \n",
       "4                      [gross, inow, month, half, ow]  \n",
       "...                                                 ...  \n",
       "4097                                   [time, question]  \n",
       "4098  [morning, couple, question, renewal, premium, ...  \n",
       "4099  [rate, bit, anomaly, drop, property, year, fac...  \n",
       "4100                     [closing, remark, follow, day]  \n",
       "4101                                [today, conference]  \n",
       "\n",
       "[4102 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use apply to get the token of the paragraph\n",
    "paragraphs_df['token'] = clean_tokens_noun(paragraphs_df['paragraph_noun'])\n",
    "print(len(paragraphs_df))\n",
    "paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>company_name</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>paragraph_clean</th>\n",
       "      <th>paragraph_noun</th>\n",
       "      <th>token</th>\n",
       "      <th>word_count</th>\n",
       "      <th>characters_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Should we get started? So good morning, everyo...</td>\n",
       "      <td>Should we get started? So good morning everyon...</td>\n",
       "      <td>morning result presentation format year fund ...</td>\n",
       "      <td>[morning, result, presentation, format, year, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "      <td>There are also a number of my executive team a...</td>\n",
       "      <td>number team non - exec morning</td>\n",
       "      <td>[number, team, exec, morning]</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Please do look them up over coee at the end.</td>\n",
       "      <td>Please do look them up over coee at the end</td>\n",
       "      <td>end</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>So the rst six months. It's fair to say that ...</td>\n",
       "      <td>So the rst six months It's fair to say that w...</td>\n",
       "      <td>month period uncertainty environment trade rel...</td>\n",
       "      <td>[month, period, uncertainty, environment, trad...</td>\n",
       "      <td>15</td>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>New gross inows for the six months was GBP 7....</td>\n",
       "      <td>New gross inows for the six months was GBP  b...</td>\n",
       "      <td>gross inow month % half ow</td>\n",
       "      <td>[gross, inow, month, half, ow]</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>We have time for one more question. It will --...</td>\n",
       "      <td>We have time for one more question It will -- ...</td>\n",
       "      <td>time question line line</td>\n",
       "      <td>[time, question]</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Hi, good morning and thanks for squeezing me i...</td>\n",
       "      <td>Hi good morning and thanks for squeezing me in...</td>\n",
       "      <td>morning thank couple line question renewal pre...</td>\n",
       "      <td>[morning, couple, question, renewal, premium, ...</td>\n",
       "      <td>43</td>\n",
       "      <td>260</td>\n",
       "      <td>13</td>\n",
       "      <td>6.046512</td>\n",
       "      <td>3.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>rate home bit anomaly drop property year fact ...</td>\n",
       "      <td>[rate, bit, anomaly, drop, property, year, fac...</td>\n",
       "      <td>39</td>\n",
       "      <td>251</td>\n",
       "      <td>11</td>\n",
       "      <td>6.435897</td>\n",
       "      <td>3.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Thank you. I will turn the call to Ms. Goldste...</td>\n",
       "      <td>Thank you I will turn the call to Ms Goldstein...</td>\n",
       "      <td>call closing remark follow up day</td>\n",
       "      <td>[closing, remark, follow, day]</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4101</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>This concludes today's conference call. We tha...</td>\n",
       "      <td>This concludes today's conference call We than...</td>\n",
       "      <td>today conference call line</td>\n",
       "      <td>[today, conference]</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4102 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_name        date  \\\n",
       "0     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "1     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "2     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "3     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "4     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "...                                                 ...         ...   \n",
       "4097  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4098  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4099  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4100  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "4101  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "\n",
       "                               company_name  \\\n",
       "0      St James's Place PLC (STJ LN Equity)   \n",
       "1      St James's Place PLC (STJ LN Equity)   \n",
       "2      St James's Place PLC (STJ LN Equity)   \n",
       "3      St James's Place PLC (STJ LN Equity)   \n",
       "4      St James's Place PLC (STJ LN Equity)   \n",
       "...                                     ...   \n",
       "4097  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4098  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4099  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4100  Travelers Cos Inc/The (TRV US Equity)   \n",
       "4101  Travelers Cos Inc/The (TRV US Equity)   \n",
       "\n",
       "                                              paragraph  \\\n",
       "0     Should we get started? So good morning, everyo...   \n",
       "1     There are also a number of my executive team a...   \n",
       "2         Please do look them up over coee at the end.   \n",
       "3     So the rst six months. It's fair to say that ...   \n",
       "4     New gross inows for the six months was GBP 7....   \n",
       "...                                                 ...   \n",
       "4097  We have time for one more question. It will --...   \n",
       "4098  Hi, good morning and thanks for squeezing me i...   \n",
       "4099  I did notice that the renewal rate in home did...   \n",
       "4100  Thank you. I will turn the call to Ms. Goldste...   \n",
       "4101  This concludes today's conference call. We tha...   \n",
       "\n",
       "                                        paragraph_clean  \\\n",
       "0     Should we get started? So good morning everyon...   \n",
       "1     There are also a number of my executive team a...   \n",
       "2          Please do look them up over coee at the end   \n",
       "3     So the rst six months It's fair to say that w...   \n",
       "4     New gross inows for the six months was GBP  b...   \n",
       "...                                                 ...   \n",
       "4097  We have time for one more question It will -- ...   \n",
       "4098  Hi good morning and thanks for squeezing me in...   \n",
       "4099  I did notice that the renewal rate in home did...   \n",
       "4100  Thank you I will turn the call to Ms Goldstein...   \n",
       "4101  This concludes today's conference call We than...   \n",
       "\n",
       "                                         paragraph_noun  \\\n",
       "0     morning result presentation format year fund ...   \n",
       "1                        number team non - exec morning   \n",
       "2                                                   end   \n",
       "3     month period uncertainty environment trade rel...   \n",
       "4                          gross inow month % half ow   \n",
       "...                                                 ...   \n",
       "4097                            time question line line   \n",
       "4098  morning thank couple line question renewal pre...   \n",
       "4099  rate home bit anomaly drop property year fact ...   \n",
       "4100                  call closing remark follow up day   \n",
       "4101                         today conference call line   \n",
       "\n",
       "                                                  token  word_count  \\\n",
       "0     [morning, result, presentation, format, year, ...          10   \n",
       "1                         [number, team, exec, morning]           4   \n",
       "2                                                    []           0   \n",
       "3     [month, period, uncertainty, environment, trad...          15   \n",
       "4                      [gross, inow, month, half, ow]           5   \n",
       "...                                                 ...         ...   \n",
       "4097                                   [time, question]           2   \n",
       "4098  [morning, couple, question, renewal, premium, ...          43   \n",
       "4099  [rate, bit, anomaly, drop, property, year, fac...          39   \n",
       "4100                     [closing, remark, follow, day]           4   \n",
       "4101                                [today, conference]           2   \n",
       "\n",
       "      characters_count  sentence_count  avg_word_length  avg_sentence_length  \n",
       "0                   68               6         6.800000             1.666667  \n",
       "1                   21               2         5.250000             2.000000  \n",
       "2                    0               2              NaN             0.000000  \n",
       "3                  120               9         8.000000             1.666667  \n",
       "4                   22               4         4.400000             1.250000  \n",
       "...                ...             ...              ...                  ...  \n",
       "4097                12               5         6.000000             0.400000  \n",
       "4098               260              13         6.046512             3.307692  \n",
       "4099               251              11         6.435897             3.545455  \n",
       "4100                22               6         5.500000             0.666667  \n",
       "4101                15               4         7.500000             0.500000  \n",
       "\n",
       "[4102 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create column for length analysis outcome\n",
    "paragraphs_df['word_count'] = paragraphs_df[\"token\"].apply(lambda x: len(x))\n",
    "paragraphs_df['characters_count'] = paragraphs_df[\"token\"].apply(lambda x: sum(len(word) for word in x))\n",
    "paragraphs_df['sentence_count'] = paragraphs_df['paragraph'].apply(lambda x: len(str(x).split(\".\")))\n",
    "paragraphs_df['avg_word_length'] = paragraphs_df['characters_count'] / paragraphs_df['word_count']\n",
    "paragraphs_df['avg_sentence_length'] = paragraphs_df['word_count'] / paragraphs_df['sentence_count']\n",
    "paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2715"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclude rows where sentence_count is less than 3\n",
    "paragraphs_df = paragraphs_df[paragraphs_df['sentence_count'] > 3]\n",
    "# Reset the index\n",
    "paragraphs_df = paragraphs_df.reset_index(drop=True)\n",
    "# Display the length of the DataFrame\n",
    "len(paragraphs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>company_name</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>paragraph_clean</th>\n",
       "      <th>paragraph_noun</th>\n",
       "      <th>token</th>\n",
       "      <th>word_count</th>\n",
       "      <th>characters_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Should we get started? So good morning, everyo...</td>\n",
       "      <td>Should we get started? So good morning everyon...</td>\n",
       "      <td>morning result presentation format year fund ...</td>\n",
       "      <td>[format, fund, ow, nancial, development, out...</td>\n",
       "      <td>10</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>So the rst six months. It's fair to say that ...</td>\n",
       "      <td>So the rst six months It's fair to say that w...</td>\n",
       "      <td>month period uncertainty environment trade rel...</td>\n",
       "      <td>[uncertainty, environment, trade, relationship...</td>\n",
       "      <td>15</td>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>New gross inows for the six months was GBP 7....</td>\n",
       "      <td>New gross inows for the six months was GBP  b...</td>\n",
       "      <td>gross inow month % half ow</td>\n",
       "      <td>[gross, inow, half, ow]</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Looking back just ve years ago, gross ows fo...</td>\n",
       "      <td>Looking back just ve years ago gross ows for...</td>\n",
       "      <td>year ow year -year period compound growth % a...</td>\n",
       "      <td>[ow, compound, growth, annum]</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Importantly, the continued strong retention of...</td>\n",
       "      <td>Importantly the continued strong retention of ...</td>\n",
       "      <td>retention client fund inow period % fund mana...</td>\n",
       "      <td>[retention, client, fund, inow, fund, basis, ...</td>\n",
       "      <td>27</td>\n",
       "      <td>183</td>\n",
       "      <td>8</td>\n",
       "      <td>6.777778</td>\n",
       "      <td>3.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>We have time for one more question. It will --...</td>\n",
       "      <td>We have time for one more question It will -- ...</td>\n",
       "      <td>time question line line</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Hi, good morning and thanks for squeezing me i...</td>\n",
       "      <td>Hi good morning and thanks for squeezing me in...</td>\n",
       "      <td>morning thank couple line question renewal pre...</td>\n",
       "      <td>[couple, renewal, premium, change, auto, state...</td>\n",
       "      <td>43</td>\n",
       "      <td>260</td>\n",
       "      <td>13</td>\n",
       "      <td>6.046512</td>\n",
       "      <td>3.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>rate home bit anomaly drop property year fact ...</td>\n",
       "      <td>[anomaly, drop, property, progress, insurance,...</td>\n",
       "      <td>39</td>\n",
       "      <td>251</td>\n",
       "      <td>11</td>\n",
       "      <td>6.435897</td>\n",
       "      <td>3.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Thank you. I will turn the call to Ms. Goldste...</td>\n",
       "      <td>Thank you I will turn the call to Ms Goldstein...</td>\n",
       "      <td>call closing remark follow up day</td>\n",
       "      <td>[closing, remark, follow]</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>This concludes today's conference call. We tha...</td>\n",
       "      <td>This concludes today's conference call We than...</td>\n",
       "      <td>today conference call line</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2715 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_name        date  \\\n",
       "0     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "1     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "2     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "3     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "4     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "...                                                 ...         ...   \n",
       "2710  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "2711  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "2712  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "2713  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "2714  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "\n",
       "                               company_name  \\\n",
       "0      St James's Place PLC (STJ LN Equity)   \n",
       "1      St James's Place PLC (STJ LN Equity)   \n",
       "2      St James's Place PLC (STJ LN Equity)   \n",
       "3      St James's Place PLC (STJ LN Equity)   \n",
       "4      St James's Place PLC (STJ LN Equity)   \n",
       "...                                     ...   \n",
       "2710  Travelers Cos Inc/The (TRV US Equity)   \n",
       "2711  Travelers Cos Inc/The (TRV US Equity)   \n",
       "2712  Travelers Cos Inc/The (TRV US Equity)   \n",
       "2713  Travelers Cos Inc/The (TRV US Equity)   \n",
       "2714  Travelers Cos Inc/The (TRV US Equity)   \n",
       "\n",
       "                                              paragraph  \\\n",
       "0     Should we get started? So good morning, everyo...   \n",
       "1     So the rst six months. It's fair to say that ...   \n",
       "2     New gross inows for the six months was GBP 7....   \n",
       "3     Looking back just ve years ago, gross ows fo...   \n",
       "4     Importantly, the continued strong retention of...   \n",
       "...                                                 ...   \n",
       "2710  We have time for one more question. It will --...   \n",
       "2711  Hi, good morning and thanks for squeezing me i...   \n",
       "2712  I did notice that the renewal rate in home did...   \n",
       "2713  Thank you. I will turn the call to Ms. Goldste...   \n",
       "2714  This concludes today's conference call. We tha...   \n",
       "\n",
       "                                        paragraph_clean  \\\n",
       "0     Should we get started? So good morning everyon...   \n",
       "1     So the rst six months It's fair to say that w...   \n",
       "2     New gross inows for the six months was GBP  b...   \n",
       "3     Looking back just ve years ago gross ows for...   \n",
       "4     Importantly the continued strong retention of ...   \n",
       "...                                                 ...   \n",
       "2710  We have time for one more question It will -- ...   \n",
       "2711  Hi good morning and thanks for squeezing me in...   \n",
       "2712  I did notice that the renewal rate in home did...   \n",
       "2713  Thank you I will turn the call to Ms Goldstein...   \n",
       "2714  This concludes today's conference call We than...   \n",
       "\n",
       "                                         paragraph_noun  \\\n",
       "0     morning result presentation format year fund ...   \n",
       "1     month period uncertainty environment trade rel...   \n",
       "2                          gross inow month % half ow   \n",
       "3     year ow year -year period compound growth % a...   \n",
       "4     retention client fund inow period % fund mana...   \n",
       "...                                                 ...   \n",
       "2710                            time question line line   \n",
       "2711  morning thank couple line question renewal pre...   \n",
       "2712  rate home bit anomaly drop property year fact ...   \n",
       "2713                  call closing remark follow up day   \n",
       "2714                         today conference call line   \n",
       "\n",
       "                                                  token  word_count  \\\n",
       "0     [format, fund, ow, nancial, development, out...          10   \n",
       "1     [uncertainty, environment, trade, relationship...          15   \n",
       "2                             [gross, inow, half, ow]           5   \n",
       "3                        [ow, compound, growth, annum]           7   \n",
       "4     [retention, client, fund, inow, fund, basis, ...          27   \n",
       "...                                                 ...         ...   \n",
       "2710                                                 []           2   \n",
       "2711  [couple, renewal, premium, change, auto, state...          43   \n",
       "2712  [anomaly, drop, property, progress, insurance,...          39   \n",
       "2713                          [closing, remark, follow]           4   \n",
       "2714                                                 []           2   \n",
       "\n",
       "      characters_count  sentence_count  avg_word_length  avg_sentence_length  \n",
       "0                   68               6         6.800000             1.666667  \n",
       "1                  120               9         8.000000             1.666667  \n",
       "2                   22               4         4.400000             1.250000  \n",
       "3                   36               4         5.142857             1.750000  \n",
       "4                  183               8         6.777778             3.375000  \n",
       "...                ...             ...              ...                  ...  \n",
       "2710                12               5         6.000000             0.400000  \n",
       "2711               260              13         6.046512             3.307692  \n",
       "2712               251              11         6.435897             3.545455  \n",
       "2713                22               6         5.500000             0.666667  \n",
       "2714                15               4         7.500000             0.500000  \n",
       "\n",
       "[2715 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the words that are too general\n",
    "general_words = [\n",
    "    'afternoon', 'morning', 'conference', 'today', 'lady', 'gentleman', 'presentation',\n",
    "    'question', 'answer', 'slide', 'mm', 'mm_mm', 'guy', 'sir', ' ', 'ytd', 'host_sir',\n",
    "    'bb', 'ty', 'word', 'year', 'quer', 'month', 'period', 'day', 'time', 'result',\n",
    "    'investor', 'week', 'update', 'business', 'lot', 'ratio', 'rate', 'quarter',\n",
    "    'number', 'point', 'term', 'thing', 'level', 'bit', 'sort', 'reason', 'management',\n",
    "    'fact', 'case', 'area', 'people', 'sense', 'item', 'issue', 'market', 'meeting',\n",
    "    'questions', 'answers', 'managements', 'discussion', 'section', 'presentation', 'speaker', 'participant',  'afternoon', 'morning', 'conference','today','lady', 'gentleman', 'presentation',\n",
    "    'question','answer', 'slide','prot', 'eect','protability','o','dierent','eciency','gure','ination','ow','cont',\n",
    "    'mm', 'mm_mm','guy','sir',' ','host_sir','bb','ty','word', 'year', 'quer','month','period', 'day', 'time','result', 'investor','week', 'update',\n",
    "    're','dierence','dicult','benet', 'business','lot','ratio','rate','quarter', 'number', 'point', 'term', 'thing', 'level', \n",
    "    'bit', 'sort', 'reason', 'management', 'fact', 'case', 'area', 'people', 'sense', 'item', 'issue', 'market'\n",
    "]\n",
    "\n",
    "paragraphs_df['token'] = paragraphs_df['token'].apply(lambda x: [i for i in x if i not in general_words])\n",
    "paragraphs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"St James's Place PLC (STJ LN Equity)\":            word  freq\n",
       " 0        client   411\n",
       " 1        growth   272\n",
       " 2          cash   262\n",
       " 3          fund   253\n",
       " 4    investment   224\n",
       " 5          cost   217\n",
       " 6           ow   177\n",
       " 7          half   156\n",
       " 8        advice   152\n",
       " 9       partner   145\n",
       " 10       change   129\n",
       " 11      adviser   126\n",
       " 12  partnership   122\n",
       " 13      expense   113\n",
       " 14      advisor   113\n",
       " 15     guidance   110\n",
       " 16       margin   107\n",
       " 17     dividend   103\n",
       " 18       charge   102\n",
       " 19       impact    94\n",
       " 20         plan    92\n",
       " 21          tax    88\n",
       " 22        model    85\n",
       " 23  environment    80\n",
       " 25      academy    78\n",
       " 24       income    78\n",
       " 27       future    77\n",
       " 28     ination    77\n",
       " 26    gestation    77\n",
       " 29        asset    73\n",
       " 30       moment    69\n",
       " 31        basis    68\n",
       " 32         face    67\n",
       " 33    retention    66\n",
       " 34       target    64\n",
       " 35  performance    63\n",
       " 36     increase    59\n",
       " 38      pension    58\n",
       " 37        inow    58\n",
       " 39  shareholder    57\n",
       " 40      outcome    57\n",
       " 41      capital    55\n",
       " 42      element    54\n",
       " 43      service    52\n",
       " 44    condition    51\n",
       " 45   experience    51\n",
       " 46      balance    51\n",
       " 47   technology    51\n",
       " 48  opportunity    50\n",
       " 49    condence    49,\n",
       " 'Travelers Cos Inc/The (TRV US Equity)':            word  freq\n",
       " 0          loss   903\n",
       " 1       premium   571\n",
       " 2        change   493\n",
       " 3       pricing   379\n",
       " 4         trend   377\n",
       " 5        income   348\n",
       " 6        impact   298\n",
       " 7        growth   282\n",
       " 8      increase   276\n",
       " 9    investment   266\n",
       " 10         auto   265\n",
       " 11      renewal   259\n",
       " 12    retention   250\n",
       " 13       return   228\n",
       " 14      segment   218\n",
       " 15     property   216\n",
       " 16     exposure   203\n",
       " 17        price   202\n",
       " 18         book   197\n",
       " 19  environment   192\n",
       " 20    portfolio   190\n",
       " 21       worker   183\n",
       " 22  improvement   179\n",
       " 23     ination   174\n",
       " 24       margin   173\n",
       " 25     severity   170\n",
       " 26         comp   168\n",
       " 27    liability   166\n",
       " 28      product   164\n",
       " 29      capital   162\n",
       " 30        basis   162\n",
       " 31        state   162\n",
       " 32        claim   159\n",
       " 33     industry   157\n",
       " 34          cat   155\n",
       " 35  catastrophe   154\n",
       " 36        share   153\n",
       " 37         cost   153\n",
       " 38         risk   151\n",
       " 39    frequency   150\n",
       " 40      expense   149\n",
       " 41        datum   148\n",
       " 42         view   145\n",
       " 43      weather   144\n",
       " 44      reserve   142\n",
       " 45          tax   138\n",
       " 46     customer   134\n",
       " 47      comment   133\n",
       " 48  development   131\n",
       " 49      account   129}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a dictionary to hold word frequencies for each company\n",
    "company_word_freq = {}\n",
    "\n",
    "# Iterate over each unique company\n",
    "for company in paragraphs_df['company_name'].unique():\n",
    "    # Filter the DataFrame for the current company\n",
    "    company_df = paragraphs_df[paragraphs_df['company_name'] == company]\n",
    "\n",
    "    # Combine tokens into a single list for the current company\n",
    "    docs_tokens = []\n",
    "    for tokens in company_df['token']:\n",
    "        docs_tokens.extend(tokens)\n",
    "    \n",
    "    # Calculate word frequency for the current company\n",
    "    word_freq = Counter(docs_tokens).most_common(50)\n",
    "    company_word_freq[company] = word_freq\n",
    "\n",
    "# Create a DataFrame for each company's word frequencies\n",
    "company_word_freq_dfs = {}\n",
    "for company, word_freq in company_word_freq.items():\n",
    "    company_word_freq_dfs[company] = pd.DataFrame(word_freq, columns=['word', 'freq']).sort_values(by='freq', ascending=False)\n",
    "\n",
    "# Display top 50 word frequencies for each company\n",
    "company_word_freq_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 50 most frequent words\n",
    "# Initialize a dictionary to hold word frequencies for each company\n",
    "company_word_freq = {}\n",
    "\n",
    "# Iterate over each unique company\n",
    "for company in paragraphs_df['company_name'].unique():\n",
    "    # Filter the DataFrame for the current company\n",
    "    company_df = paragraphs_df[paragraphs_df['company_name'] == company]\n",
    "\n",
    "    # Combine tokens into a single list for the current company\n",
    "    docs_tokens = []\n",
    "    for tokens in company_df['token']:\n",
    "        docs_tokens.extend(tokens)\n",
    "    \n",
    "    # Calculate word frequency for the current company\n",
    "    word_freq = Counter(docs_tokens).most_common(50)\n",
    "    company_word_freq[company] = [word for word, freq in word_freq]\n",
    "\n",
    "# Remove the most frequent words from the token column\n",
    "mdy_list, tmp = [], []\n",
    "for _, row in paragraphs_df.iterrows():\n",
    "    company = row['company_name']\n",
    "    review = row['token']\n",
    "    word_list = company_word_freq[company]\n",
    "    tmp = [word for word in review if word not in word_list]\n",
    "    mdy_list.append(tmp)\n",
    "\n",
    "paragraphs_df['token'] = mdy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2715\n",
      "2232\n"
     ]
    }
   ],
   "source": [
    "print(len(paragraphs_df))\n",
    "paragraphs_df['token_len'] = paragraphs_df['token'].apply(lambda x: len(x))\n",
    "# Drop rows where the length of the token is less than 2\n",
    "paragraphs_df = paragraphs_df[paragraphs_df['token_len'] > 2]\n",
    "print(len(paragraphs_df))\n",
    "# Recreate 'docs_tokens' from 'paragraphs_df'\n",
    "temp_token = paragraphs_df['token'] # .apply(remove_brackets)\n",
    "docs_tokens = []\n",
    "for i in temp_token:\n",
    "    docs_tokens.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Saving Final DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script processes the cleaned and tokenised earnings call transcripts and sorts them by file name and date. It then groups the data by file name, date and company name, aggregating tokens and paragraphs. The tokens are flattened, and separate DataFrames are created for each company. Finally, the script saves each company's DataFrame to a CSV file in a designated directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>company_name</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>paragraph_clean</th>\n",
       "      <th>paragraph_noun</th>\n",
       "      <th>token</th>\n",
       "      <th>word_count</th>\n",
       "      <th>characters_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Should we get started? So good morning, everyo...</td>\n",
       "      <td>Should we get started? So good morning everyon...</td>\n",
       "      <td>morning result presentation format year fund ...</td>\n",
       "      <td>[format, nancial, development, outlook]</td>\n",
       "      <td>10</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>So the rst six months. It's fair to say that ...</td>\n",
       "      <td>So the rst six months It's fair to say that w...</td>\n",
       "      <td>month period uncertainty environment trade rel...</td>\n",
       "      <td>[uncertainty, trade, relationship, wealth, bac...</td>\n",
       "      <td>15</td>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Importantly, the continued strong retention of...</td>\n",
       "      <td>Importantly the continued strong retention of ...</td>\n",
       "      <td>retention client fund inow period % fund mana...</td>\n",
       "      <td>[track, record, start, track, record, percenta...</td>\n",
       "      <td>27</td>\n",
       "      <td>183</td>\n",
       "      <td>8</td>\n",
       "      <td>6.777778</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>So why is this? Well rst and foremost, St. Ja...</td>\n",
       "      <td>So why is this? Well rst and foremost St Jame...</td>\n",
       "      <td>relationship business % ow client introductio...</td>\n",
       "      <td>[relationship, introduction, life, journey, a...</td>\n",
       "      <td>26</td>\n",
       "      <td>175</td>\n",
       "      <td>8</td>\n",
       "      <td>6.730769</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>Where gross ows can be impacted is with discr...</td>\n",
       "      <td>Where gross ows can be impacted is with discr...</td>\n",
       "      <td>ow investment say bonus proceed disposal asse...</td>\n",
       "      <td>[bonus, proceed, disposal, sale, individual, u...</td>\n",
       "      <td>18</td>\n",
       "      <td>122</td>\n",
       "      <td>7</td>\n",
       "      <td>6.777778</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Okay, got it. And maybe secondly, I'm curious ...</td>\n",
       "      <td>Okay got it And maybe secondly I'm curious as ...</td>\n",
       "      <td>re - underwriting book takeaway month integration</td>\n",
       "      <td>[underwriting, takeaway, integration]</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>We're feeling really good about bringing and l...</td>\n",
       "      <td>We're feeling really good about bringing and l...</td>\n",
       "      <td>capability organization quality protability b...</td>\n",
       "      <td>[capability, organization, quality, protabili...</td>\n",
       "      <td>12</td>\n",
       "      <td>97</td>\n",
       "      <td>7</td>\n",
       "      <td>8.083333</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Hi, good morning and thanks for squeezing me i...</td>\n",
       "      <td>Hi good morning and thanks for squeezing me in...</td>\n",
       "      <td>morning thank couple line question renewal pre...</td>\n",
       "      <td>[couple, adequacy, couple, think, decline, pol...</td>\n",
       "      <td>43</td>\n",
       "      <td>260</td>\n",
       "      <td>13</td>\n",
       "      <td>6.046512</td>\n",
       "      <td>3.307692</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>I did notice that the renewal rate in home did...</td>\n",
       "      <td>rate home bit anomaly drop property year fact ...</td>\n",
       "      <td>[anomaly, drop, progress, insurance, coverage,...</td>\n",
       "      <td>39</td>\n",
       "      <td>251</td>\n",
       "      <td>11</td>\n",
       "      <td>6.435897</td>\n",
       "      <td>3.545455</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>Thank you. I will turn the call to Ms. Goldste...</td>\n",
       "      <td>Thank you I will turn the call to Ms Goldstein...</td>\n",
       "      <td>call closing remark follow up day</td>\n",
       "      <td>[closing, remark, follow]</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2232 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_name        date  \\\n",
       "0     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "1     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "4     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "5     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "6     cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "...                                                 ...         ...   \n",
       "2708  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "2709  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "2711  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "2712  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "2713  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "\n",
       "                               company_name  \\\n",
       "0      St James's Place PLC (STJ LN Equity)   \n",
       "1      St James's Place PLC (STJ LN Equity)   \n",
       "4      St James's Place PLC (STJ LN Equity)   \n",
       "5      St James's Place PLC (STJ LN Equity)   \n",
       "6      St James's Place PLC (STJ LN Equity)   \n",
       "...                                     ...   \n",
       "2708  Travelers Cos Inc/The (TRV US Equity)   \n",
       "2709  Travelers Cos Inc/The (TRV US Equity)   \n",
       "2711  Travelers Cos Inc/The (TRV US Equity)   \n",
       "2712  Travelers Cos Inc/The (TRV US Equity)   \n",
       "2713  Travelers Cos Inc/The (TRV US Equity)   \n",
       "\n",
       "                                              paragraph  \\\n",
       "0     Should we get started? So good morning, everyo...   \n",
       "1     So the rst six months. It's fair to say that ...   \n",
       "4     Importantly, the continued strong retention of...   \n",
       "5     So why is this? Well rst and foremost, St. Ja...   \n",
       "6     Where gross ows can be impacted is with discr...   \n",
       "...                                                 ...   \n",
       "2708  Okay, got it. And maybe secondly, I'm curious ...   \n",
       "2709  We're feeling really good about bringing and l...   \n",
       "2711  Hi, good morning and thanks for squeezing me i...   \n",
       "2712  I did notice that the renewal rate in home did...   \n",
       "2713  Thank you. I will turn the call to Ms. Goldste...   \n",
       "\n",
       "                                        paragraph_clean  \\\n",
       "0     Should we get started? So good morning everyon...   \n",
       "1     So the rst six months It's fair to say that w...   \n",
       "4     Importantly the continued strong retention of ...   \n",
       "5     So why is this? Well rst and foremost St Jame...   \n",
       "6     Where gross ows can be impacted is with discr...   \n",
       "...                                                 ...   \n",
       "2708  Okay got it And maybe secondly I'm curious as ...   \n",
       "2709  We're feeling really good about bringing and l...   \n",
       "2711  Hi good morning and thanks for squeezing me in...   \n",
       "2712  I did notice that the renewal rate in home did...   \n",
       "2713  Thank you I will turn the call to Ms Goldstein...   \n",
       "\n",
       "                                         paragraph_noun  \\\n",
       "0     morning result presentation format year fund ...   \n",
       "1     month period uncertainty environment trade rel...   \n",
       "4     retention client fund inow period % fund mana...   \n",
       "5     relationship business % ow client introductio...   \n",
       "6     ow investment say bonus proceed disposal asse...   \n",
       "...                                                 ...   \n",
       "2708  re - underwriting book takeaway month integration   \n",
       "2709  capability organization quality protability b...   \n",
       "2711  morning thank couple line question renewal pre...   \n",
       "2712  rate home bit anomaly drop property year fact ...   \n",
       "2713                  call closing remark follow up day   \n",
       "\n",
       "                                                  token  word_count  \\\n",
       "0              [format, nancial, development, outlook]          10   \n",
       "1     [uncertainty, trade, relationship, wealth, bac...          15   \n",
       "4     [track, record, start, track, record, percenta...          27   \n",
       "5     [relationship, introduction, life, journey, a...          26   \n",
       "6     [bonus, proceed, disposal, sale, individual, u...          18   \n",
       "...                                                 ...         ...   \n",
       "2708              [underwriting, takeaway, integration]           5   \n",
       "2709  [capability, organization, quality, protabili...          12   \n",
       "2711  [couple, adequacy, couple, think, decline, pol...          43   \n",
       "2712  [anomaly, drop, progress, insurance, coverage,...          39   \n",
       "2713                          [closing, remark, follow]           4   \n",
       "\n",
       "      characters_count  sentence_count  avg_word_length  avg_sentence_length  \\\n",
       "0                   68               6         6.800000             1.666667   \n",
       "1                  120               9         8.000000             1.666667   \n",
       "4                  183               8         6.777778             3.375000   \n",
       "5                  175               8         6.730769             3.250000   \n",
       "6                  122               7         6.777778             2.571429   \n",
       "...                ...             ...              ...                  ...   \n",
       "2708                40               7         8.000000             0.714286   \n",
       "2709                97               7         8.083333             1.714286   \n",
       "2711               260              13         6.046512             3.307692   \n",
       "2712               251              11         6.435897             3.545455   \n",
       "2713                22               6         5.500000             0.666667   \n",
       "\n",
       "      token_len  \n",
       "0             4  \n",
       "1             7  \n",
       "4             9  \n",
       "5            12  \n",
       "6            10  \n",
       "...         ...  \n",
       "2708          3  \n",
       "2709          7  \n",
       "2711         14  \n",
       "2712         15  \n",
       "2713          3  \n",
       "\n",
       "[2232 rows x 13 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by the file_name and date \n",
    "fidf = paragraphs_df.sort_values(by=['file_name', 'date'])\n",
    "fidf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "      <th>company_name</th>\n",
       "      <th>token</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 201...</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>[format, nancial, development, outlook, uncer...</td>\n",
       "      <td>[Should we get started? So good morning, every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 202...</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>[decade, format, nancial, matter, note, execu...</td>\n",
       "      <td>[Good morning everyone. It's half ten, so we s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 202...</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>[agenda, introduction, nancial, outlook, mome...</td>\n",
       "      <td>[Given COVID-19, today's presentation has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 202...</td>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>[pandemic, review, focus, summary, individual,...</td>\n",
       "      <td>[Good morning. I hope you're keeping safe and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 202...</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>[outlook, session, team, combination, assumpti...</td>\n",
       "      <td>[Good morning, and welcome to our 2021 Interim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 202...</td>\n",
       "      <td>2022-02-24</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>[height, resilience, circumstance, agility, em...</td>\n",
       "      <td>[In 2020, at the height of the pandemic, St. J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 202...</td>\n",
       "      <td>2022-07-28</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>[agenda, gure, topic, medium, prospect, outlo...</td>\n",
       "      <td>[Good morning, and welcome to our 2022 Half-Ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 202...</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>[start, globe, backdrop, uncertainty, history,...</td>\n",
       "      <td>[Good morning, and welcome to our Full-Year Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 202...</td>\n",
       "      <td>2023-07-27</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>[session, format, gure, regime, progress, pri...</td>\n",
       "      <td>[This morning's session will follow a familiar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cleaned_St James_s Place PLC Earnings Call 202...</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>St James's Place PLC (STJ LN Equity)</td>\n",
       "      <td>[pleasure, ceo, agenda, sight, headline, volum...</td>\n",
       "      <td>[Good morning, everyone. It's my pleasure to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 201...</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 201...</td>\n",
       "      <td>2019-04-18</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 201...</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[press, release, supplement, webcast, material...</td>\n",
       "      <td>[Thank you so much, and good morning. Welcome ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2020-10-20</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[press, release, supplement, webcast, traveler...</td>\n",
       "      <td>[Thank you. Good morning and welcome to Travel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[press, release, supplement, website, traveler...</td>\n",
       "      <td>[Thank you. Good morning and welcome to Travel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2021-10-19</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[press, release, supplement, webcast, traveler...</td>\n",
       "      <td>[Thank you. Good morning, and welcome to Trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2021-01-21</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning and thank you for holding. Welco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2021-04-20</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[mode, session, operator, instruction, press, ...</td>\n",
       "      <td>[Thank you for standing by and welcome to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2021-07-20</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2022-10-19</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2022-01-20</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen, and welco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2022-04-19</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[press, release, supplement, webcast, traveler...</td>\n",
       "      <td>[Thank you. Good morning, and welcome to Trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2023-10-18</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2023-04-19</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2023-07-20</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-01-19</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cleaned_Travelers Cos IncThe Earnings Call 202...</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>Travelers Cos Inc/The (TRV US Equity)</td>\n",
       "      <td>[completion, remark, instruction, session, rem...</td>\n",
       "      <td>[Good morning, ladies and gentlemen. Welcome t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            file_name        date  \\\n",
       "0   cleaned_St James_s Place PLC Earnings Call 201...  2019-07-31   \n",
       "1   cleaned_St James_s Place PLC Earnings Call 202...  2020-02-27   \n",
       "2   cleaned_St James_s Place PLC Earnings Call 202...  2020-07-28   \n",
       "3   cleaned_St James_s Place PLC Earnings Call 202...  2021-02-25   \n",
       "4   cleaned_St James_s Place PLC Earnings Call 202...  2021-07-28   \n",
       "5   cleaned_St James_s Place PLC Earnings Call 202...  2022-02-24   \n",
       "6   cleaned_St James_s Place PLC Earnings Call 202...  2022-07-28   \n",
       "7   cleaned_St James_s Place PLC Earnings Call 202...  2023-02-28   \n",
       "8   cleaned_St James_s Place PLC Earnings Call 202...  2023-07-27   \n",
       "9   cleaned_St James_s Place PLC Earnings Call 202...  2024-02-28   \n",
       "10  cleaned_Travelers Cos IncThe Earnings Call 201...  2019-10-22   \n",
       "11  cleaned_Travelers Cos IncThe Earnings Call 201...  2019-04-18   \n",
       "12  cleaned_Travelers Cos IncThe Earnings Call 201...  2019-07-23   \n",
       "13  cleaned_Travelers Cos IncThe Earnings Call 202...  2020-10-20   \n",
       "14  cleaned_Travelers Cos IncThe Earnings Call 202...  2020-01-23   \n",
       "15  cleaned_Travelers Cos IncThe Earnings Call 202...  2020-04-21   \n",
       "16  cleaned_Travelers Cos IncThe Earnings Call 202...  2021-10-19   \n",
       "17  cleaned_Travelers Cos IncThe Earnings Call 202...  2021-01-21   \n",
       "18  cleaned_Travelers Cos IncThe Earnings Call 202...  2021-04-20   \n",
       "19  cleaned_Travelers Cos IncThe Earnings Call 202...  2021-07-20   \n",
       "20  cleaned_Travelers Cos IncThe Earnings Call 202...  2022-10-19   \n",
       "21  cleaned_Travelers Cos IncThe Earnings Call 202...  2022-01-20   \n",
       "22  cleaned_Travelers Cos IncThe Earnings Call 202...  2022-04-19   \n",
       "23  cleaned_Travelers Cos IncThe Earnings Call 202...  2022-07-21   \n",
       "24  cleaned_Travelers Cos IncThe Earnings Call 202...  2023-10-18   \n",
       "25  cleaned_Travelers Cos IncThe Earnings Call 202...  2023-01-24   \n",
       "26  cleaned_Travelers Cos IncThe Earnings Call 202...  2023-04-19   \n",
       "27  cleaned_Travelers Cos IncThe Earnings Call 202...  2023-07-20   \n",
       "28  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-01-19   \n",
       "29  cleaned_Travelers Cos IncThe Earnings Call 202...  2024-04-17   \n",
       "\n",
       "                             company_name  \\\n",
       "0    St James's Place PLC (STJ LN Equity)   \n",
       "1    St James's Place PLC (STJ LN Equity)   \n",
       "2    St James's Place PLC (STJ LN Equity)   \n",
       "3    St James's Place PLC (STJ LN Equity)   \n",
       "4    St James's Place PLC (STJ LN Equity)   \n",
       "5    St James's Place PLC (STJ LN Equity)   \n",
       "6    St James's Place PLC (STJ LN Equity)   \n",
       "7    St James's Place PLC (STJ LN Equity)   \n",
       "8    St James's Place PLC (STJ LN Equity)   \n",
       "9    St James's Place PLC (STJ LN Equity)   \n",
       "10  Travelers Cos Inc/The (TRV US Equity)   \n",
       "11  Travelers Cos Inc/The (TRV US Equity)   \n",
       "12  Travelers Cos Inc/The (TRV US Equity)   \n",
       "13  Travelers Cos Inc/The (TRV US Equity)   \n",
       "14  Travelers Cos Inc/The (TRV US Equity)   \n",
       "15  Travelers Cos Inc/The (TRV US Equity)   \n",
       "16  Travelers Cos Inc/The (TRV US Equity)   \n",
       "17  Travelers Cos Inc/The (TRV US Equity)   \n",
       "18  Travelers Cos Inc/The (TRV US Equity)   \n",
       "19  Travelers Cos Inc/The (TRV US Equity)   \n",
       "20  Travelers Cos Inc/The (TRV US Equity)   \n",
       "21  Travelers Cos Inc/The (TRV US Equity)   \n",
       "22  Travelers Cos Inc/The (TRV US Equity)   \n",
       "23  Travelers Cos Inc/The (TRV US Equity)   \n",
       "24  Travelers Cos Inc/The (TRV US Equity)   \n",
       "25  Travelers Cos Inc/The (TRV US Equity)   \n",
       "26  Travelers Cos Inc/The (TRV US Equity)   \n",
       "27  Travelers Cos Inc/The (TRV US Equity)   \n",
       "28  Travelers Cos Inc/The (TRV US Equity)   \n",
       "29  Travelers Cos Inc/The (TRV US Equity)   \n",
       "\n",
       "                                                token  \\\n",
       "0   [format, nancial, development, outlook, uncer...   \n",
       "1   [decade, format, nancial, matter, note, execu...   \n",
       "2   [agenda, introduction, nancial, outlook, mome...   \n",
       "3   [pandemic, review, focus, summary, individual,...   \n",
       "4   [outlook, session, team, combination, assumpti...   \n",
       "5   [height, resilience, circumstance, agility, em...   \n",
       "6   [agenda, gure, topic, medium, prospect, outlo...   \n",
       "7   [start, globe, backdrop, uncertainty, history,...   \n",
       "8   [session, format, gure, regime, progress, pri...   \n",
       "9   [pleasure, ceo, agenda, sight, headline, volum...   \n",
       "10  [completion, remark, instruction, session, rem...   \n",
       "11  [completion, remark, instruction, session, rem...   \n",
       "12  [press, release, supplement, webcast, material...   \n",
       "13  [press, release, supplement, webcast, traveler...   \n",
       "14  [press, release, supplement, website, traveler...   \n",
       "15  [completion, remark, instruction, session, rem...   \n",
       "16  [press, release, supplement, webcast, traveler...   \n",
       "17  [completion, remark, instruction, session, rem...   \n",
       "18  [mode, session, operator, instruction, press, ...   \n",
       "19  [completion, remark, instruction, session, rem...   \n",
       "20  [completion, remark, instruction, session, rem...   \n",
       "21  [completion, remark, instruction, session, rem...   \n",
       "22  [press, release, supplement, webcast, traveler...   \n",
       "23  [completion, remark, instruction, session, rem...   \n",
       "24  [completion, remark, instruction, session, rem...   \n",
       "25  [completion, remark, instruction, session, rem...   \n",
       "26  [completion, remark, instruction, session, rem...   \n",
       "27  [completion, remark, instruction, session, rem...   \n",
       "28  [completion, remark, instruction, session, rem...   \n",
       "29  [completion, remark, instruction, session, rem...   \n",
       "\n",
       "                                            paragraph  \n",
       "0   [Should we get started? So good morning, every...  \n",
       "1   [Good morning everyone. It's half ten, so we s...  \n",
       "2   [Given COVID-19, today's presentation has been...  \n",
       "3   [Good morning. I hope you're keeping safe and ...  \n",
       "4   [Good morning, and welcome to our 2021 Interim...  \n",
       "5   [In 2020, at the height of the pandemic, St. J...  \n",
       "6   [Good morning, and welcome to our 2022 Half-Ye...  \n",
       "7   [Good morning, and welcome to our Full-Year Re...  \n",
       "8   [This morning's session will follow a familiar...  \n",
       "9   [Good morning, everyone. It's my pleasure to t...  \n",
       "10  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "11  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "12  [Thank you so much, and good morning. Welcome ...  \n",
       "13  [Thank you. Good morning and welcome to Travel...  \n",
       "14  [Thank you. Good morning and welcome to Travel...  \n",
       "15  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "16  [Thank you. Good morning, and welcome to Trave...  \n",
       "17  [Good morning and thank you for holding. Welco...  \n",
       "18  [Thank you for standing by and welcome to the ...  \n",
       "19  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "20  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "21  [Good morning, ladies and gentlemen, and welco...  \n",
       "22  [Thank you. Good morning, and welcome to Trave...  \n",
       "23  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "24  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "25  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "26  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "27  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "28  [Good morning, ladies and gentlemen. Welcome t...  \n",
       "29  [Good morning, ladies and gentlemen. Welcome t...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy fidf to fidfbase and select specific columns\n",
    "fidfbase = fidf.copy()\n",
    "fidfbase = fidfbase[['file_name', 'date', 'company_name', 'token', 'paragraph', 'paragraph_clean', 'paragraph_noun']]\n",
    "\n",
    "# Groupby 'file_name', 'date', 'company_name' and aggregate tokens and paragraphs\n",
    "token_grouped = fidfbase.groupby(['file_name', 'date', 'company_name'])['token'].apply(list).reset_index()\n",
    "paragraph_grouped = fidfbase.groupby(['file_name', 'date', 'company_name'])['paragraph'].apply(list).reset_index()\n",
    "paragraph_clean_grouped = fidfbase.groupby(['file_name', 'date', 'company_name'])['paragraph_clean'].apply(list).reset_index()\n",
    "paragraph_noun_grouped = fidfbase.groupby(['file_name', 'date', 'company_name'])['paragraph_noun'].apply(list).reset_index()\n",
    "\n",
    "# Merge token and paragraph lists into a single DataFrame\n",
    "token_paragraph_merged = pd.merge(token_grouped, paragraph_grouped, on=['file_name', 'date', 'company_name'])\n",
    "\n",
    "# Define function to flatten lists and remove brackets\n",
    "def flatten_list(x):\n",
    "    flattened_list = [item for sublist in x for item in sublist]\n",
    "    return flattened_list\n",
    "\n",
    "# Apply flatten_list to 'token' column in token_paragraph_merged\n",
    "token_paragraph_merged['token'] = token_paragraph_merged['token'].apply(flatten_list)\n",
    "token_paragraph_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "St_Jamess_df: (845, 4)\n",
      "Travelers_Cos_df: (1387, 4)\n",
      "None\n",
      "Saved St_Jamess_df to company_csvs\\St_Jamess_df.csv\n",
      "Saved Travelers_Cos_df to company_csvs\\Travelers_Cos_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Group by 'company_name' to create separate DataFrames for each company\n",
    "company_dfs = {}\n",
    "for company in fidfbase['company_name'].unique():\n",
    "    company_df = fidfbase[fidfbase['company_name'] == company][['file_name', 'date', 'token', 'paragraph']]\n",
    "    company_name_cleaned = \"_\".join(company.split()[:2]).replace('(', '').replace(')', '').replace('.', '').replace(',', '').replace(\"'\", \"\") + '_df'\n",
    "    company_dfs[company_name_cleaned] = company_df\n",
    "\n",
    "# Example of accessing a specific company DataFrame\n",
    "for company_name, df in company_dfs.items():\n",
    "    print(f\"{company_name}: {df.shape}\")\n",
    "\n",
    "# Function to get DataFrame by company name\n",
    "def get_company_df(company_name):\n",
    "    company_name_cleaned = \"_\".join(company_name.split()[:2]).replace('(', '').replace(')', '').replace('.', '').replace(',', '').replace(\"'\", \"\") + '_df'\n",
    "    return company_dfs.get(company_name_cleaned, None)\n",
    "\n",
    "# Example usage:\n",
    "df = get_company_df('Traveler Cos')\n",
    "print(df)\n",
    "\n",
    "# Save each company's DataFrame to a CSV file\n",
    "output_dir = \"company_csvs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for company_name, df in company_dfs.items():\n",
    "    csv_path = os.path.join(output_dir, f\"{company_name}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {company_name} to {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
